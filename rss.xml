<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Async Stream RSS Feed]]></title><description><![CDATA[Async Stream RSS Feed]]></description><link>https://jhellerstein.github.io/blog</link><generator>GatsbyJS</generator><lastBuildDate>Wed, 11 Feb 2026 21:12:20 GMT</lastBuildDate><item><title><![CDATA[Codegen Meets Distributed Reality]]></title><description><![CDATA[AI is about to write most of the code in the world. Most of the code in the world participates in a distributed system.
 And distributed code‚Ä¶]]></description><link>https://jhellerstein.github.io/blog/codegen-reality/</link><guid isPermaLink="false">https://jhellerstein.github.io/blog/codegen-reality/</guid><pubDate>Wed, 11 Feb 2026 00:00:00 GMT</pubDate><content:encoded>
AI is about to write most of the code in the world.  
Most of the code in the world participates in a distributed system.  
And distributed code is where our worst bugs live.

That leads to a simple thesis: **AI should aim at languages where many distributed bugs can‚Äôt be expressed at all.**

The collision course sketched above is the story of the next decade of systems engineering. We can celebrate the productivity‚Äîand we should‚Äîbut we also need to ask a harder question: **what language constructs do we want AI to aim at**? If the target is ordinary imperative code, we‚Äôll get ordinary distributed bugs, only faster. If the target is a framework that makes many of those bugs impossible to express, we get something very different.

That‚Äôs the argument of this post: **the right target for AI is a framework where distributed systems bugs are largely impossible to express**. [Hydro](https://hydro.run) is one such framework for distributed programs. The alternative‚Äîplain code plus increasingly heroic testing‚Äîis reasonable, but it carries structural downsides that matter more as AI scales.

---

## The ground we‚Äôre standing on

Today, every ‚Äúsimple‚Äù app is a distributed system: a phone talking to a cloud service, which talks to other services, queues, and databases. Distribution isn‚Äôt an advanced topic anymore; it‚Äôs the floor.

At the same time, LLMs are getting good at turning English intent into working programs. That‚Äôs intoxicating. You describe what you want, the machine fills in the ceremony, and a service appears.

But distributed bugs don‚Äôt respect ceremony. They show up after retries, batching, and unlucky orderings nobody pictured when the code was written. They can sleep in stored state for months and then wake up on a Tuesday afternoon. LLMs are excellent at happy paths; the corners of distributed behavior are not happy places.

We‚Äôre about to try and automate the hardest part of our craft.

---

## Testing is necessary ‚Äî and bounded

The natural response is: test harder. And we‚Äôve never had better tools. Bounded model checkers like TLA+, Alloy, Jepsen, and Antithesis can explore real systems brutally and systematically. That work is a triumph of our field.

But bounded model checking sounds like proof, and it isn‚Äôt; it‚Äôs bounded exploration. These tools are disciplined fuzzers: they examine a large space, yet still only a slice of what production will see‚Äîendless request diversity, long-lived state, retries, flaky networks, and occasionally adversaries. The part you don‚Äôt explore dwarfs the part you do.

Model checkers don‚Äôt guarantee correctness.  
They guarantee you didn‚Äôt find a bug in the executions you looked at.

That‚Äôs not a criticism; it‚Äôs the math of state explosion. In the cloud, your service runs for years under traffic patterns no bounded test suite can enumerate. Reality is a far more aggressive fuzzer than our fuzzers.

Which means the real question isn‚Äôt how much we test, but what surface we test against.

Bounded testing explains why we miss executions; the deeper problem is that even the executions we do see rarely reveal what went wrong.

---

## The failure mode tests can‚Äôt diagnose

Even infinite search wouldn‚Äôt fix the outages that hurt most: the ones that arise from **mismatched and unstated assumptions between components**.

One service assumes ordered delivery; another produces unordered batches. Ninety-nine percent of the time the combination works. In the one percent case, a rare permutation violates an unstated contract and the system‚Äôs brain is split for the rest of time.

A checker can show the trace where that happened.  
It can‚Äôt tell you what assumptions were made incorrectly, or what the overall intent might have been.

The problem isn‚Äôt lack of exploration. It‚Äôs that **distributed contracts have no obvious home**. The English prompts and specs talk about high-level goals. The generated imperative code talks about procedures. The contracts between components‚Äîordering, retries, idempotence, and so on‚Äîfloat in the gap between them. For an LLM, that gap is invisible, so it fills it with guesses.

---

## Target matters

**If ordinary code leaves contracts implicit, some framework has to constrain the code so those contracts become explicit and checkable.** This is where the choice of frameworks becomes decisive. Hydro takes the stance that distributed behavior should be **deterministic by default**‚Äîe.g., unordered collections cannot silently feed order-sensitive code‚Äîand that deviations from determinism should be explicit and signposted.

In Hydro:

- choices about ordering and delivery are explicit in interfaces,  
- compositions that violate those choices are compile-time type errors,  
- the few places where behavior can truly diverge are marked with a keyword: `nondet!`.

The point isn‚Äôt to outlaw uncertainty. It‚Äôs to give it a narrow scope and signpost.

**Instead of:**

&gt; Write a service that processes events and retries failures.

**You ask:**

&gt; Write a Hydro service with unordered input and at-most-once output,  
&gt; no `nondet!` blocks.

Any use of order-dependent logic then becomes a compile-time error.

The difference is not who writes the code, but what the code is allowed to say. The agent still writes code, but the tricky distributed assumptions become machine-checked facts rather than folklore. Many races and other heisenbugs simply cannot be expressed without crossing a bright line at compile time.

---

## Tradeoffs, not perfection

That promise‚Äîeliminating whole classes of heisenbugs‚Äîis deliberately narrow. Hydro does not attempt to verify full application semantics. It won‚Äôt prove your business logic is right, your protocol matches a spec, or that a system meets liveness goals. What it targets instead are the failures that bounded model checkers spend most of their time hunting: accidental nondeterminism from hidden ordering, retries, and delivery assumptions. By making those choices explicit, many of the traces checkers labor to discover simply cannot arise.

This tradeoff is deliberate. We can‚Äôt yet automatically reason about every aspect of open-ended cloud systems, but we can remove the layer of unpredictability that turns small design mistakes into sprawling search problems. The analogy is Rust: memory safety doesn‚Äôt guarantee a correct program, but it eliminates an entire class of failures so we can reason at a higher level. Hydro aims for that kind of foundation for distributed behavior.

A separate, practical wrinkle: today, models are less fluent in Hydro than in plain Rust. That will improve with better training and tooling, but even now the constraint is useful‚Äîit pushes generation toward explicit contracts instead of silent assumptions. The goal isn‚Äôt AI that writes anything; it‚Äôs AI that writes distributed systems we can reason about.

---

## Checking with a map

Testing doesn‚Äôt disappear in this world; it changes character. HydroSim‚Äîthe checker for Hydro‚Äîonly needs to explore around the `nondet!` code sites. Deterministic paths can be trusted rather than tested; the search budget goes to the real ambiguity.

In practice that means deeper exploration of actual risks in the same time, and far fewer ghost counterexamples from harmless plumbing. The conversation shifts from staring at traces to talking about the spec.

You stop asking, ‚ÄúDid we test enough?‚Äù  
You start asking, ‚ÄúDid we understand the few places where the program can diverge?‚Äù

---

## The target AI should aim at

AI will generate oceans of distributed code either way. The question is whether we point it at a coding framework where distributed bugs are easy to write, or at one where those bugs require an explicit escape hatch.

Traditional imperative code plus model checking is still a sensible baseline. It will remain indispensable for legacy systems. But as a *destination* for AI code generation, it‚Äôs a poor default. It asks us to debug our way out of problems the AI could have prevented by design, simply by picking a better target.

Better testing finds more traces; better languages make many traces unnecessary.

---

## The real opportunity

Going live in the cloud will always provide a more comprehensive test than any checker can offer. The durable response is to change how we express distributed programs so that intent is visible long before the first counterexample appears.

Most future distributed code will be AI-generated. The practical choice isn‚Äôt whether we‚Äôll test harder, but **what target we‚Äôll ask the AI to aim at**. If we aim at ordinary imperative code, we commit to finding problems after they happen. If we aim at frameworks where distributed contracts are part of the language surface, many of those outages never get a chance to exist.

Hydro is one example of that kind of target: a setting where ordering and delivery are explicit, and the few genuinely uncertain steps are clearly marked. Targets like this don‚Äôt eliminate testing; they focus it on the right questions instead of chasing shadows.

Give AI a target that reflects distributed reality, and it can help us build systems that fail less often‚Äîand for better reasons.</content:encoded></item><item><title><![CDATA[Three Lenses on Coordination]]></title><description><![CDATA[Three Sets of Specs (for Staying in Spec) In the last post I talked about how systems formalisms are like sculpting with a chisel: we‚Ä¶]]></description><link>https://jhellerstein.github.io/blog/three-specs-on-specs/</link><guid isPermaLink="false">https://jhellerstein.github.io/blog/three-specs-on-specs/</guid><pubDate>Tue, 03 Feb 2026 00:00:00 GMT</pubDate><content:encoded>
import Callout from &apos;../../../src/components/Callout&apos;
import { withPrefix } from &quot;gatsby&quot;

## Three Sets of Specs (for Staying in Spec)

In the last post I talked about how systems formalisms are like sculpting with 
a chisel: we remove behaviors we don‚Äôt want, and what remains is correct.
This subtractive view shows up as system invariants: concurrent actions
shouldn‚Äôt conflict, replicas shouldn‚Äôt drift beyond reconciliation, and
deadlocks should be impossible.

In this post we turn our attention to the work at hand:
*When is staying within spec easy, and when is it hard?*

This can be tricky to reason about because the same underlying difficulty keeps reappearing in different guises.
Sometimes it shows up as waiting. Sometimes as ordering. Sometimes as
questions about what the system exposes to observers, and when.

In this post, I‚Äôll look at these issues through three different pairs of
spectacles‚Äîthree ways engineers tend to first *notice* that staying within specification
has become tricky:

1. **Waiting**: when do we have to wait, and what are we waiting for?
2. **Ordering**: how much order do we really need to impose?
3. **Commitment**: what kinds of claims does the system expose to observers, and stick with?

We won‚Äôt do formal theory, and we won‚Äôt build protocols. The goal is to
connect these different viewpoints and show how they fit together.
Only at the end will we return to a practical question systems people
care about: when do we actually need extra machinery‚Äîand when don‚Äôt we?

---

## Lens 1: Waiting. ‚ÄúCome one vs. Come all‚Äù

We don‚Äôt like to wait in computing, but sometimes we *have* to in order to stay within spec.
Waiting adds latency and can risk unavailability or deadlock. But at bottom, there are only two fundamental *reasons* we wait:

1. Waiting for something to happen  
2. Waiting to know that nothing else will happen

### Waiting for something
The first category is the familiar one. The clearest example is a *data dependency*.
If my job is to compute the function `x + y`, I have to wait until the values of `x`
and `y` are available (and perhaps the instruction for `+`, if we‚Äôre thinking at the chip level).
That‚Äôs easy to understand, and it‚Äôs easy to implement. The computation becomes ready exactly
when its inputs arrive.

But many cases of ‚Äúwaiting for something to happen‚Äù
*feel* more complicated: they&apos;re not about passing data, they involve some
notion of &quot;control&quot; messages. Nonetheless, many of these cases fall in this same category.

Consider message acknowledgment. I send you a message:
‚Äú`msg m: 2 + 2 = 4`.‚Äù Once I receive an &quot;`ack: m`&quot; from you, I know you‚Äôve received it,
and I can proceed with that knowledge in hand‚Äîperhaps freeing the memory holding the
inputs and output. Until then, I‚Äôm waiting at your mercy. If you‚Äôre slow to respond,
I‚Äôm stuck. This can feel like a different kind of waiting‚Äîafter all, I&apos;m waiting for
‚Äúcontrol‚Äù messages that are irrelevant to the &quot;data&quot; in my computation.

But this setting is not materially different from a function call. In a single-threaded program,
if my code calls a (local) function `x + y`, I can‚Äôt proceed as if `x + y` has completed
until the function returns. The return isn‚Äôt permission or agreement; it‚Äôs evidence that the work I
depend on has finished. The distributed acknowledgment plays the same role: it‚Äôs the signal
that the step I depend on has completed.

A function return doesn‚Äôt involve another machine, but the need to wait is already there.
The distributed case doesn‚Äôt introduce a new kind of dependency‚Äîit just stretches an existing one
across a network.

These ‚Äòwaiting for something‚Äô cases are all instances of causal dependence on available evidence:
once the needed event is observed, the waiting is over.

### Waiting for nothing‚Ä¶or everyone

As a contrast, consider a different scenario: distributed termination detection.

Imagine I‚Äôve paid for a global network of machines to work on a problem
for me, and I want to know when that computation is finished.
Intuitively, the computation is done when two conditions both hold:
(i) no machine is currently taking steps on my behalf, and (ii) there
are no messages in flight that could trigger further steps.

That sounds reasonable‚Äîbut how do I establish those facts?
Who‚Äôs to say that a message isn‚Äôt delayed somewhere in the network, ready to reignite the computation?

What I‚Äôm waiting for here is very different from the earlier examples.
I‚Äôm not waiting for a particular event to arrive. I‚Äôm waiting for a
guarantee that *nothing else will happen*. Formally, what I want to know
is something like:

&gt; There exists no machine still working for me, and there exists no
&gt; message in flight on my behalf.

That kind of claim doesn‚Äôt follow from causality. Causality tells me how
events relate once they happen; it doesn‚Äôt tell me that no further events
are possible.

Put differently, ‚Äúnothing exists‚Äù is a universal claim (hello, DeMorgan!). To conclude
that no machine is still working, I need confirmation from every
machine. Waiting for nothing quietly turns into waiting to hear from
everyone.

That reframing explains why this kind of waiting feels so different. In
the earlier cases, progress was triggered by arrival. Here, progress
depends on establishing a global absence. And absences don‚Äôt announce
themselves.

This also exposes two immediate complications. First, I need to know who
counts as ‚Äúeveryone.‚Äù If machines can join dynamically, or if the set of
participants isn‚Äôt fixed, the target keeps moving. Second, even if I do
know the full roster, what happens if one machine is slow, unreachable,
or has failed? Am I allowed to proceed without hearing from it? On what
basis?

These questions simply don‚Äôt arise when waiting for something to happen.
Once an input arrives, the dependency is satisfied and computation can
move forward locally. Waiting for non-arrival, by contrast, asks the
system to make a claim about the future: that no further relevant events
will occur.

This distinction‚Äîbetween waiting for arrival and waiting for guaranteed
absence‚Äîis well known in distributed systems. It shows up in classic
work on termination detection, and more abstractly in results like CALM,
which formalize reasoning from positive evidence versus reasoning from absence.
One kind of waiting is driven by evidence. The other is driven by exhaustion.

And that‚Äôs the tension to keep in mind as we move on. Waiting for
something lets the system react. Waiting for nothing forces the
system to make a claim about the future. Ordering turns out to do the same thing.

---

## Lens 2: Order ‚Äî partial orders and the cost of total order
Ordering problems often feel different from waiting problems, but
they create pressure in a similar way.

Causal events naturally form a partial order: a DAG of events with
‚Äúhappens-before‚Äù edges. Many events are independent‚Äîthey race, overlap,
and have no inherent order between them. A partial order captures exactly
what must be ordered, and no more.

But sometimes a system asks for more than a partial order. Sometimes it
wants a *total* order: every event must be placed in a single sequence.

This comes up in very familiar places. Linearizability asks us to
explain a concurrent execution as if operations happened one at a time.
Serializability asks us to pretend that transactions ran sequentially.
In both cases, the goal is to emulate a single-threaded program.

At first glance, this may not seem like a big step. After all, many total
orders are consistent with a given partial order. Why not just pick one?

The answer is that a total order asserts strictly more than a partial one.
It doesn‚Äôt merely say ‚Äúevent a happened before event b.‚Äù
It also says that **nothing else can come between a and b**.

That second clause is a claim about unseen events.
Here, ‚Äúunseen‚Äù includes future events, and events that are delayed in reaching you.

A partial order can grow naturally as events occur. If two events are
independent, their relative order can remain unspecified, possibly
forever.

A total order removes that freedom. It forces a decision about the order
of racing events‚Äîeven when there is no causal reason to decide, and even
when not all relevant events have yet occurred.

Once such a decision is made, it cannot be taken back without revising
the history being asserted.

This is why systems that promise linearizability or serializability end
up doing so much extra work. The cost isn‚Äôt in maintaining order where
causality already demands it. The cost is in committing early, before
the system has seen enough to know which total orders will remain
compatible with future events.

That‚Äôs the tension to keep in mind. Partial orders let the system defer
decisions. Total order forces it to decide‚Äîand to live with the
consequences.

---

## Lens 3: What the system is willing to expose

So far we‚Äôve talked about waiting and ordering as properties of how a
system executes. The final lens shifts perspective: from execution to
**observation**.

In a distributed system, there is no single place where events are
observed or understood. There are many replicas, each learning about
the world at different times and in different orders. Any statement the
system exposes to the outside world has to survive that fact.

In the easy cases, this works out well.

Statements like ‚Äúthis message arrived,‚Äù or ‚Äúevent a happened before
event b,‚Äù are reports of observed evidence. They may be learned at
different times by different replicas, but they do not conflict.
Later information can add detail or context, but it will not make the
statement false.

Because of this, replicas can safely expose such facts independently.
Observers may see partial information, but what they see will only be
extended, never retracted. The system‚Äôs public story grows by
accumulating evidence.

The hard cases arise when the system exposes stronger claims.

‚ÄúNothing else will happen.‚Äù
‚ÄúThis result is final.‚Äù
‚ÄúThis is the order.‚Äù

These are not just summaries of what has been observed so far. They are
assertions that rule out future observations.

Once such a claim is made public, the system is committed to it. Future
events must be handled in a way that preserves the claim‚Äîeven if those
events have not yet been seen everywhere, or at all.

This is where replication becomes a problem.

One replica may expose a claim based on the information it has, while
another replica is still in a position to observe something that
contradicts it. An observer may learn the claim before the evidence
needed to justify it has fully propagated. At that point, the system
cannot simply ‚Äúupdate‚Äù the observer‚Äôs view without violating the
meaning of what it already said.

The same pattern we saw in the earlier lenses reappears here.

Waiting for arrival is easy because exposing evidence cannot be
invalidated by later events. Waiting for non-arrival is hard because it
exposes an absence that might still be contradicted.

Partial order is easy because exposing causal relationships can only be
strengthened by more information. Total order is hard because exposing
it rules out alternative explanations that might still be consistent
with unseen events.

Seen this way, the core question is not about waiting or ordering at
all. It is about **which kinds of statements a system can safely expose
before it has seen everything**, and which ones require extra care.

Some statements are safe to reveal as soon as they are known locally.
Others are only safe once the system has taken steps to ensure that no
future observation will contradict them.

That difference is what brings coordination into the picture.

---

## Conclusion: when you need a chisel

Throughout this post, we‚Äôve been looking at how systems stay within
spec‚Äîby ruling out behaviors they won‚Äôt allow. What the three lenses
show is that there are two very different ways this happens.

When a system reacts to evidence, the space of possible executions shrinks
as information accumulates.
This is why replicas can diverge temporarily and still reconcile.
They may see events in different orders, but as they exchange
information, their views converge. Evidence removes ambiguity on its
own, without anyone having to declare which futures are allowed.

The hard cases are different.
Here, the system wants to rule out futures that the world has not ruled
out yet. It wants to say that nothing else will happen, that a result is
final, or that a particular ordering is settled and cannot be revised.
Those futures are not impossible‚Äîthey are merely unwanted.

At that point, the system can no longer rely on monotonic accumulation
alone. The passage of events will not do the work for it. To stay within
spec, the system has to actively forbid certain futures and get all
participants to respect that decision.

That is what we usually call coordination.

Coordination is not about slowness or synchronization for its own sake.
It is the extra machinery a system needs when correctness depends on
excluding futures that might otherwise still occur.

This distinction shows up again and again: in transactions and
isolation levels, in bulk-synchronous execution, and in systems that
must act before all uncertainty is resolved. Those connections are
worth exploring, but they‚Äôre for later posts.

For now, the takeaway is this. If the futures you want to rule out will
be ruled out anyway by the arrival of information, staying within spec
is easy. If they won‚Äôt be, the system will need help.</content:encoded></item><item><title><![CDATA[Algorithms Compute Functions. Systems Make Promises.]]></title><description><![CDATA[I've been having a lot of fun working on Hydro at AWS this fall. But over the winter break I stepped away from coding and had the space to‚Ä¶]]></description><link>https://jhellerstein.github.io/blog/algs-systems/</link><guid isPermaLink="false">https://jhellerstein.github.io/blog/algs-systems/</guid><pubDate>Tue, 30 Dec 2025 00:00:00 GMT</pubDate><content:encoded>I&apos;ve been having a lot of fun working on [Hydro](https://github.com/hydro-project/hydro)
at AWS this fall.
But over the winter break I stepped away from coding and had the space to think ... mostly about
fundamentals of long-time obsessions around concurrency, coordination
and correctness specs.

I don&apos;t have specific results to report here (yet :-)). Instead, I thought I&apos;d share a conceptual contrast
 that‚Äôs been clarifying a lot of my recent thinking


&gt; **Algorithms compute functions.**  
&gt; **Systems make promises about preventing futures.**

That sounds negative, but it&apos;s pretty on point. Most systems guarantees are prohibitions: 
states that may never occur, histories that must not be observed, interleavings that are not allowed, etc.
And in many systems, *the promising is an ongoing process*: the set of promises evolves continually based
on the system&apos;s observable behavior in the outside world. 

A system‚Äôs job is not to produce a single correct output and stop; it‚Äôs to keep running *without
ever crossing certain semantic lines*.

This difference explains a lot of friction we see when we move ideas back and forth between algorithms and systems.

## What Algorithms Do

When we write an algorithm, we have a pretty clear contract:
	- You give me an input (possibly complex, possibly huge).
	- I produce an output.
	- Correctness means the output corresponds to the function we set out to compute.

We might worry about efficiency, approximation, or even randomness ‚Äî but fundamentally
we‚Äôre evaluating a function. The algorithm is allowed to wait to see all of its input;
it doesn‚Äôt have to make irreversible choices until it‚Äôs ready.

There‚Äôs a clarity to this world. It‚Äôs like working at a potter&apos;s wheel with the clay in front of you.

That‚Äôs a comfortable world. Meaning is fixed by the problem statement; the algorithm‚Äôs job is to compute it.

## What Systems Do
Systems are &quot;online&quot; -- they exist in time. They interact with an environment (clients, networks, the
physical world). They necessarily observe *partial information*. They do not get to pause the world
indefinitely while they think. What ‚Äúcorrectness‚Äù means here isn‚Äôt producing the right answer once ‚Äî it‚Äôs never
violating your invariants, ever.

Promises are a qualitatively different thing than answers: they‚Äôre commitments about what futures remain possible.
I.e. systems prevent things from happening. *Systems rule out futures*.

In many cases (transaction commit, leader election, scheduling one process before another),
once a system rules out a future, that exclusion is permanent. The system has committed.

To take an example from my home field of databases, if we have two conflicting concurrent
transactions $T_1, T_2$, the database system must make a hard
exclusionary promise: if it lets $T_1$ commit, it promises that $T_2$&apos;s writes
will never be visible to other transactions. The system &quot;picks a world of histories&quot; that excludes
histories where $T_2$&apos;s writes are visible.

## Why ‚ÄúActing Before Understanding‚Äù Matters

This is where things get interesting.

Algorithms are allowed to wait until everything is well-defined. Systems often aren‚Äôt.

A database must commit or abort a transaction before it knows what other concurrent transactions may
want to do next. A distributed service must respond before it knows which messages
are delayed. An online learning system must deploy a model before it has seen tomorrow‚Äôs data.

In each case, the system must act before the world is fully known. That action whittles away possibilities 
until only an acceptable outcome is revealed. The meaning of the system
is defined by removing the negative space, being left with only observable behavior
that satisfies the invariants. 

&gt; **Systems construct meaning like sculptors make art:  
&gt; by chiseling away the negative space in a world of possibilities.**

This is the common structure behind a lot of familiar problems:
	- coordination
	- consistency levels
	- isolation anomalies
	- optimistic execution
	- speculative and asynchronous learning

They look different on the surface. Underneath, they‚Äôre all about deciding which
futures to forbid, and when.

I plan to keep chipping away at this idea in future posts.</content:encoded></item><item><title><![CDATA[Schema Evolution, Career Edition]]></title><description><![CDATA[üîî For years I‚Äôve been asserting that it‚Äôs time to change the way we write software‚Ä¶]]></description><link>https://jhellerstein.github.io/blog/hello-aws/</link><guid isPermaLink="false">https://jhellerstein.github.io/blog/hello-aws/</guid><pubDate>Thu, 25 Sep 2025 00:00:00 GMT</pubDate><content:encoded>
import { withPrefix } from &quot;gatsby&quot;

## üîî &lt;i&gt;I&apos;ve Joined AWS ... with one foot still at Berkeley.&lt;/i&gt;

For years I‚Äôve been asserting that it‚Äôs time to change the way we write software, and stop pretending we&apos;re programming one machine at a time. With coding agents emerging as a disruptive force, I‚Äôm convinced the moment has come‚Äîit‚Äôs time to get real!

In pursuit of that vision, I have joined AWS to help shape the next generation of distributed software development. I‚Äôm not coming alone‚Äîthree other core members of the [Hydro](https://hydro.run) project have joined with me (newly-minted Dr. [Shadaj Laddad](https://www.shadaj.me/), [Mingwei Samuel](https://www.linkedin.com/in/mingweisamuel) and [Lucky Katahanas](https://www.linkedin.com/in/luckykatahanas)). Together, we‚Äôll continue advancing Hydro as an open-source initiative, collaborating with colleagues at institutions like Berkeley and Princeton.

## Distributed Coding Challenges, Post-LLMs
The timing of this move is no coincidence. LLM-driven coding agents are changing the landscape by radically lowering the burden of authoring code‚Äîeven in new languages and frameworks. But LLMs are undisciplined by definition, and nobody likes an undisciplined software engineer. LLMs need to be coupled with core computer science techniques for ensuring safety of code. This is the ‚Äúneuro-symbolic‚Äù vision of AI, which pairs up ‚Äúguessers‚Äù (LLMs in this case) and ‚Äúcheckers‚Äù (deterministic deduction, as in type systems and declarative query languages.)

Modern languages like Rust are helpful checkers in this environment, by addressing longstanding challenges like memory safety without introducing performance overheads. Hydro takes this further, allowing developers to write Rust functions that can span multiple machines, with type system guarantees of *distributed safety*. Hydro code is guaranteed to be safe in the face of non-determinism inherent in modern distributed systems‚Äã‚Äîfrom communication delays to network interleavings‚Äîbefore it will even compile. And developers still get all the goodies they like about Rust, from IDE support to LLM assistance.

## AWS
Choosing AWS was strategic. I‚Äôm excited to be part of an organization that leads in spec-driven development (hello, [Kiro](https://kiro.dev)!) and the [use of formal methods for proving correctness of critical infrastructure](https://cacm.acm.org/practice/systems-correctness-practices-at-amazon-web-services/), while contributing in a major way to the Rust open source community. The folks who run the world&apos;s biggest cloud have a vested interest in extremely reliable distributed systems, making them an ideal champion for bringing years of our research into practice.

## UC Berkeley
I‚Äôm still maintaining a role as Professor of the Graduate School at Berkeley, enabling me to continue steering the open Hydro research agenda and co-advising students alongside brilliant colleagues like [Natacha Crooks](https://nacrooks.github.io/) and [Max Willsey](https://www.mwillsey.com/).

While I generally [keep CALM](https://cacm.acm.org/research/keeping-calm/), I‚Äôm fired up to pursue this moment at AWS.
</content:encoded></item><item><title><![CDATA[Tips on Research]]></title><description><![CDATA[As a complement to my last post, I thought I'd write a follow-up with some constructive advice. No point complaining about negativity if you‚Ä¶]]></description><link>https://jhellerstein.github.io/blog/research-tips/</link><guid isPermaLink="false">https://jhellerstein.github.io/blog/research-tips/</guid><pubDate>Fri, 27 Jun 2025 00:00:00 GMT</pubDate><content:encoded>
As a complement to my last post, I thought I&apos;d write a follow-up with some constructive advice. 
No point complaining about negativity if you don&apos;t offer some positive alternatives!

# Five Tips for Doing Your Best Research

Rather than opine about research topics, which are always changing, I thought I&apos;d offer advice on research processes. No doubt a lot of this is conventional wisdom, but I came to these conclusions based on my own career experience. Maybe some of this will be useful to those of you who are earlier on in your journey. Your mileage may vary.


1. **Follow Your Muse, but Evaluate Humbly**. Pursue topics that fascinate you; the best work comes from enthusiasm. But also build in mechanisms that force you to be clear-headed about assessing the impact of your work over time. Evaluate on a timescale that suits your career and work (5-year windows align well with PhD student lifecycles), and humbly move on from topics where the impact isn&apos;t what you wanted. Time spent in industry‚Äîespecially entrepreneurship‚Äîis helpful in learning this discipline; the market is an efficient (heartless!) teacher of lessons in adaptability and humility, as anyone whose startup has &quot;pivoted&quot; will share. Changing course can be scary, but sometimes it sets you free to shine again.

2. **Explore and Exploit**: After my first few years in research, I started deliberately trying to invest my research time in a portfolio of a few &quot;long plays&quot; (in my case, applying declarative/dataflow programming to a variety of other domains) and a larger number of shorter but diverse excursions. Reinforcement learning folks might call this &quot;explore vs. exploit&quot;, and clearly one should do some of both. Connecting back to the previous point, it&apos;s the long plays that benefit the most from deliberate reflection; the short ones have agility built in.

3. **Pick Topics that Leverage Your Working Style**: Pick topic areas that favor a research style that fits your personality. For example, working on AI-related topics right now demands that you produce a lot of quick-fire results in a tightly competitive arena. That requires an restless competitive streak: reading a ton, being quick to change, and doing research in relatively short bites on very popular topics. By contrast, meaningful systems work requires an investment of many years of engineering and teamwork, during which there will be fallow periods of publication. In that space, you need patience, good taste and self-confidence, because you don&apos;t get to change your mind nearly as often. There&apos;s no right answer here‚Äîyou might enjoy a mixture, and one style is not inherently better than another. But it&apos;s good to make sure that your personal working style matches your topic selection. In your early days, finding role models in the literature can help‚Äîagain, one role model may be quite different from another, so choose what appeals to you and reassess over time.

4. **Collaborate with Complementary Experts**. It&apos;s good intellectual nutrition to collaborate with experts in a variety of other fields. A lot of the best research opportunities are at the seams between traditional fields, and it&apos;s more educational and fun to work with people who know and think about things different than your experience.
For faculty, I suggest spending ~5 years co-advising Ph.D. students with people in another field, whether that&apos;s across or outside CS. Then consider trying a new collaboration in a different field thereafter. 
Doing this will broaden your horizons, and teach you more about your own strengths: the ones that come from your education in your home field, and the ones that come from your personal research style and talents.

5. **For Topic Selection, Do Your (Qualitative) Research**. This is maybe the least common advice in this list. To choose important problems‚Äîespecially in user-facing areas‚Äîconsider learning how to do qualitative research methods like interview studies, to figure out empirically what problems will have impact. I&apos;ve been involved in doing this twice (once for [Enterprise Data Analytics](http://vis.stanford.edu/files/2012-EnterpriseAnalysisInterviews-VAST.pdf), and more recently for [Machine Learning Engineering](https://dl.acm.org/doi/abs/10.1145/3653697)). In both cases, the interview studies led to interesting and highly relevant follow-on constructive work, and the papers describing the studies and results were themselves well received: both won awards. Be aware: this is a lot of work to do well! (To be clear, in the cases above that work was not done by me‚Äîit was done by amazing students like [Sean Kandel](https://en.wikipedia.org/wiki/Sean_Kandel), [Rolando Garcia](https://rlnsanz.github.io/) and [Shreya Shankar](https://www.sh-reya.com/)).  Note that versions of this approach are now standard practice for entrepreneurs, and that&apos;s an area where we can learn from industry practices as well as social scientists. See the book [The Mom Test](https://www.momtestbook.com/) for a distilled discussion.

# Caveat
I&apos;ll close with a standard warning I give when people ask me for advice on starting companies. Most entrepreneurs wildly overfit their advice to their own narrow experience. Some folks who dish out advice are more self-aware about that than others. The overfitting is more extreme in the startup world than academia, but the theme remains. 

So... take my advice for what it&apos;s worth (hint: this blog is free!) and go ask a lot of other people for theirs too.
</content:encoded></item><item><title><![CDATA[An Optimist's Reflection on SIGMOD]]></title><description><![CDATA[I write this note on my way home from the annual ACM SIGMOD conference, one of the top venues for data management research. In this post I‚Ä¶]]></description><link>https://jhellerstein.github.io/blog/sigmod-optimism/</link><guid isPermaLink="false">https://jhellerstein.github.io/blog/sigmod-optimism/</guid><pubDate>Fri, 27 Jun 2025 00:00:00 GMT</pubDate><content:encoded>I write this note on my way home from the annual [ACM SIGMOD](https://2025.sigmod.org) conference,
one of the top venues for data management research. In this post I want to reflect on
one of the best-attended sessions, a [panel discussion](https://arxiv.org/abs/2504.08948v1) 
that continued what I think is a sorry tradition of needless negativity for our burgeoning field.

## Starting Positive
Before I get into that, though, let me say that in general I thought this year&apos;s conference was great! 
I am no doubt biased, having spent many many hours leading the program committee with my wonderful co-Chair 
and friend [Azza Abouzied](https://azzaabouzied.com/); I&apos;d like to think that time was well spent. 
Also, numbers for submissions, acceptances, and even attendance were all way up this year‚Äîthings continue 
to heat up despite the challenges with visas, geopolitics and funding.

Meanwhile, at the conference I got genuinely 
excited by new ideas on a bunch of occasions. Some of the papers this year seem to
crack open big long-standing problems‚Äîespecially in the area of [Query Optimization](https://2025.sigmod.org/program_full_detail.shtml#sigmod_research_9__query_optimization),
a longstanding interest of mine. We may be at a pivot point in that 50-year-old grand challenge!

On a personal level I actually find the hubbub of conferences a bit overwhelming, but they do get my brain juices
flowing. And it was good to see old friends, reinforce connections, and talk with new people. I also 
had good fun playing the plastic cornet to mark the ends of the breaks. (Points to folks who  
identify the names of some of the musical excerpts in the comments below!)

## Another Eeyore Panel...Sigh
This year&apos;s SIGMOD, once again, featured a very well-attended, soul-searching
(read: self-flagellating)
panel on the direction and value of academic research in our field.
The panel followed the form of a recurring shtick in our community: stirring discussion 
by positing a crisis of irrelevance. 

Mike Stonebraker used to be the main culprit of these panels,
trying to provoke conversation by saying things like &quot;my VC friends think academic computer
science is like a beached whale. Discuss.&quot; Mike is a hero, but I never thought this was one of his
better leadership strategies‚Äîespecially with respect to encouraging 
young people entering the field.

This year&apos;s panel was more diplomatically managed by two emerging leaders 
who I admire a lot, [Eugene Wu](http://www.cs.columbia.edu/~ewu/) and [Raul Castro Fernandez](https://raulcastrofernandez.com/). But the 
spirit and structure was familiar. Panelists were encouraged to make &quot;provocative&quot;
assertions, which repeatedly included things like &quot;data stakeholders don&apos;t want to buy what we&apos;re selling&quot; and
&quot;database systems are already fast, so stop working on performance&quot;. 

For both of these, at face value the answer is clearly &quot;Duh... No!&quot;, with billions of 
dollars in revenue and cost-savings as evidence. Of course there was a kernel of nuanced 
useful advice in these statements (&quot;be responsive to a wider variety of user needs&quot;, &quot;we can focus on more
topics than performance&quot;), but it was buried in a pile of polemic hogwash encouraged
by the format.

These panels frustrate me. They 
inject negativity and false narratives into the discourse in the name of being
&quot;provocative&quot;, and I get 
impatient because the time spent debating the foolishness could be devoted to far 
more constructive purposes.

## Don&apos;t Worry, Be Happy! ü§†

For my young colleagues who work on data management research, here&apos;s the good news, and 
the *truth:*  data management is an especially rich environment to build a career.
To begin with, there are plenty of well-defined, important and challenging technical problems 
right in front of our noses. Many of these problems have clear economic benefit to established 
vendors, so you can find out about them pretty easily, and do work that gets quick recognition.
Whether these are longstanding problems to be chipped away at, or perennial challenges that keep
shifting with workloads or tech trends, there&apos;s no shortage of well-known topics where you can make 
your name and make a big difference.

Even better: from there, the upside potential is much higher. If history is any guide, there will
be an ongoing stream of unexpected new directions in data management, many of which
will be career-defining. As you gain confidence, go explore! Dream big! Think outside the box!
Choose your cliche...the point is, in the data field the odds of outsized success are bigger than
in most scientific and engineering disciplines. Frankly, we&apos;re spoiled with relevance and opportunity.

And to my senior colleagues let me say this: Enough with the Chicken-Licken-meets-Eeyore act.
Spread some sunshine to the junior folks. To rattle off one more cliche: Lead, Follow or Get Out of the Way!

In the spirit of being constructive, in my next post I&apos;ll share some actionable ideas that I&apos;ve 
found useful for choosing impactful and personally rewarding research.
</content:encoded></item><item><title><![CDATA[CRDTs #4: Convergence, Determinism, Lower Bounds and Inflation]]></title><description><![CDATA[The CRDT literature sometimes leaves room for mathematical ambiguity. Maybe because the bulk of the work tends to be targeted at systems‚Ä¶]]></description><link>https://jhellerstein.github.io/blog/crdt-inflationary/</link><guid isPermaLink="false">https://jhellerstein.github.io/blog/crdt-inflationary/</guid><pubDate>Thu, 29 May 2025 00:00:00 GMT</pubDate><content:encoded>import Callout from &apos;../../../src/components/Callout&apos;
import { withPrefix } from &quot;gatsby&quot;


The CRDT literature sometimes leaves room for mathematical ambiguity. Maybe because the bulk of the work tends to be targeted at systems researchers and developers, like a lot of work on eventual consistency.

The discussion below untangles three subtle but important ideas in CRDT design, which turn out to be interrelated:

1. Determinism vs Convergence guarantees
2. Early `read`s: lower bounds or not?
3. Algebraic property requirements for `update` functions.

&lt;Callout type=&quot;warning&quot;&gt;
The CRDT guarantee of *strong eventual consistency does **not** guarantee determinism!*
If you want your CRDTs to be deterministic, or you want to treat CRDT `read`s as lower bounds, then your `update` functions must be **inflationary**.
&lt;/Callout&gt;

&gt; This is the 4th post in a [series](../crdt-intro/) I&apos;m doing on CRDTs. This one is a bit more technical and narrow than my previous CRDT posts, but practitioners should still know about the main conclusions. This post also contains a small formal result that seems novel -- Strong Eventual Consistency does not guarantee determinism! I&apos;d be curious to hear about prior work that makes this point.

---

## Definitions First

Before we get into the details, let&apos;s be a bit careful with our terminology. I&apos;ll assume you&apos;ve seen CRDTs before, but I&apos;ll review the key points while trying to keep this breezy.

### Join Semi-Lattices

You likely remember that CRDTs are related to *lattices*¬†in abstract algebra. Let&apos;s review that.

Let‚Äôs suppose we have a set of possible states $S$, equipped with a **join operator** $\sqcup: S \times S \to S$. The join is required to satisfy three properties:
* **Idempotent**: $x \sqcup x = x$
* **Commutative**: $x \sqcup y = y \sqcup x$
* **Associative**: $x \sqcup (y \sqcup z) = (x \sqcup y) \sqcup z$

If you have a set of states and a binary operator satisfying these properties, you have a **join-semilattice**.

From this operator, we can define a **partial order**:

$$
x \leq y \quad \text{iff} \quad x \sqcup y = y
$$

This means that applying join never forgets anything. It only moves ‚Äúupward‚Äù in the order induced by join.

### **From Join Semi-Lattices to CRDTs**

A CRDT is essentially a replicated join semi-lattice. 
To the core mathematical definition we add one more API, an **`update` function** that mutates the state of the CRDT (while of course staying within the state space $S$).‚ÄØ As we&apos;ll see below, these functions can be the source of nondeterminism depending on their algebraic properties. Also, the CRDT literature tends to refer to the join operator as `merge`. (That works for me, as it disambiguates the CRDT term from the unrelated relational join operator from databases.)

Next, we assume an asynchronous¬†**network dissemination service**  that periodically sends a snapshot of one node&apos;s state to another. When state arrives at a destination node, its local CRDT state is mutated to reflect the `merge` of the current state with the message.‚ÄØ
We&apos;ll assume that every node eventually `merge`s data from every other node directly or indirectly (transitively).

That&apos;s all we need, at the level of detail we&apos;ll pursue here. However, the literature on CRDTs typically highlights a subclass of CRDTs called &quot;op-based&quot; CRDTs. You can read about those in an [earlier post](../crdt-turtles) and I&apos;ll have more to say about them [below](#op-based).

Given this background, let‚Äôs move on to define a key property of functions on the lattice domain.

### Inflationary
We say that a function $f:S \rightarrow S$ over an ordered domain $S$ is **inflationary** if its output is never smaller than its input:
$$
x \leq f(x)
$$

For CRDT discussion, we assume the domain $S$ is the same as that of our semi-lattice, and the partial order $\leq$  is the one from our semi-lattice definition. Inflationary functions never take us &quot;down&quot; in the lattice order.


&lt;Callout type=&quot;info&quot;&gt;
Let‚Äôs start with something reassuring: the join operator $x \sqcup y$ is always inflationary in each of its arguments.
&lt;/Callout&gt;

Why?
$x \leq x \sqcup y$ because $x \sqcup y$ is an upper bound for $x$.

This is one reason why many CRDT papers don‚Äôt emphasize inflationarity: they tend to focus on `merge` operations, and `merge` ($\sqcup$) satisfies the property automatically.

Our issue is going to be the `update` function each node may run between `merge`s. More on that shortly.

But first, let&apos;s define the standard guarantee that CRDTs are designed to provide.


### Strong Eventual Consistency (SEC)

The canonical correctness guarantee for CRDTs is **Strong Eventual Consistency (SEC)**, as defined by [Shapiro et al](https://link.springer.com/chapter/10.1007/978-3-642-24550-3_29). It requires that, eventually, all replicas converge to the same state, provided they have seen the same set of `update`s.

The SEC condition includes three properties:

1. **Eventual delivery**: All `update`s eventually reach all replicas.
2. **Termination**: All operations eventually complete.
3. **Strong convergence**: If two replicas have received the same set of `update`s, they will reach the same state.

Note that SEC says nothing about ordering of operations; in particular it¬†**does not constrain the interleaving of `update` and `merge`**. Given that dissemination is assumed to be asynchronous, a replica might send its current state before or after applying an `update`. We‚Äôll see how this matters next.

---

## SEC Does Not Guarantee Determinism
We tend to think of convergence as a property that avoids non-determinism: after all, in a convergent replica system, all the replicas agree on the outcome. But that doesn&apos;t actually mean that they agree on a *deterministic* outcome!

As defined, Strong Eventual Consistency only says that replicas converge *within* a run. It says nothing about whether different executions ‚Äî with the same set of `updates` and nodes ‚Äî produce the same final result.

Consider the following function:

$$
\text{DropTop}(x) = \begin{cases}
  a &amp; \text{if } x = \top \\
  x &amp; \text{otherwise}
\end{cases}
$$

The $\top$ (&quot;top&quot;) symbol is the standard lattice notation for topmost state in a finite lattice (i.e. the unique join of all the states).

$\text{DropTop}$ is not inflationary: it &quot;drops&quot; from $\top$ to a lower value $a$. If a user applies it once at node `A`, the final result depends on whether `A` sends its state to `B` *before or after* applying the `update`.

**Example: Two Possible Outcomes**

Let $S = \{\bot, a, \top\}$, with $\bot &lt; a &lt; \top$.

* Node A starts at $\top$
* Node B starts at $\bot$
* A single user applies $\text{DropTop}$ once on node A

&lt;div className=&quot;diagram-two-floats&quot;&gt;
  &lt;div className=&quot;diagram-float&quot;&gt;
    &lt;img src={withPrefix(&quot;/img/crdt-run1.svg&quot;)} alt=&quot;Outcome 1&quot; style={{ width: &quot;85%&quot;, display: &quot;block&quot; }} /&gt;
    &lt;div className=&quot;diagram-caption&quot;&gt;Run 1&lt;/div&gt;
  &lt;/div&gt;
  &lt;div className=&quot;diagram-float&quot;&gt;
    &lt;img src={withPrefix(&quot;/img/crdt-run2.svg&quot;)} alt=&quot;Outcome 2&quot; style={{ width: &quot;85%&quot;, display: &quot;block&quot; }} /&gt;
    &lt;div className=&quot;diagram-caption&quot;&gt;Run 2&lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;

**Run 1**: send first, then `update`

* A **sends** $\top$ to B
* B **merges**: $\bot \sqcup \top = \top$
* A **applies** $\text{DropTop}: \top \mapsto a$
* B **sends** $\top$ to A
* A **merges**: $a \sqcup \top = \top$
* Final state: A = $\top$, B = $\top$

**Run 2**: `update` first, then send
* A **applies** $\text{DropTop}: \top \mapsto a$
* A **sends** $a$ to B
* B **merges**: $\bot \sqcup a = a$
* Final state: A = $a$, B = $a$

Same updates. Same nodes. Same messages. Different orders. Two different converged states.

---

## Inflationarity is Needed for Deterministic Convergence

As we&apos;ve seen, non-inflationary functions like $\text{DropTop}$  may non-deterministically drive the CRDT into one of many possible converged states, which depend on the interleaving of `update`s and messages.

If all `update`s are inflationary, then once a state has moved &quot;up&quot; the lattice, it never moves back down ‚Äî and that ensures:

* Each replica&apos;s state moves up the lattice.
* Merging always moves replicas up in the lattice.
* The final state is the least upper bound of all `update`s.

That last point ensures **determinism**: the final result depends only on the set of `update`s, not their timing.

&lt;Callout type=&quot;warning&quot;&gt;
**Inflationary `update`s are required for deterministic convergence in CRDTs.**
&lt;/Callout&gt;
Non-inflationary `update`s will still converge to some common state, but the choice of converged state will be non-deterministic.

---

## Non-Inflationary Updates Spoil Lower Bounds
In [the previous post](../crdt-dont-read) I talked about how it&apos;s unsafe to `read` the naked state of a CRDT, but at least 
a &quot;well-behaved&quot; CRDT provides lower bounds. Well, in addition to providing non-deterministic outcomes,
**non-inflationary `update`s are &quot;ill-behaved&quot; and do *not* provide lower bounds**.

Imagine you&apos;re writing application logic that reads from a CRDT value during execution.

If the `update` function is inflationary, you have a guarantee: the current state always provides a **lower bound** of the final state. That makes it safe to make assumptions like:

* &quot;This user will never lose this permission.&quot;
* &quot;This counter will only increase from here.&quot;

But if `update`s are **not** inflationary, those assumptions no longer hold. You might see a high value now ‚Äî and a lower one later ‚Äî because your node applied a non-inflationary `update`.

That means your application can&apos;t treat CRDT reads as stable observations in any sense. It must treat every intermediate value as potentially temporary.

&lt;Callout type=&quot;danger&quot;&gt;
When using non-inflationary `update` functions, CRDT `read` values can be completely arbitrary. Inflationary `update` ensures that CRDT `read` is a lower bound on the final converged state, which is a common assumption in the CRDT literature.
&lt;/Callout&gt;

&lt;br/&gt;&lt;br/&gt;

### Ensuring Inflationarity
There is a simple hack to ensure inflationarity, by changing the implementation of the `update` API, or removing it entirely.

The change is as follows. When a user invokes `update(x)`, we do not directly mutate the CRDT state to `x`; rather, we mutate the state to `merge(update(x))`. Because `merge` is inflationary (as discussed above), the effect of updates in this implementation is always inflationary.
The result is determinism and guaranteed lower bounds, at the cost of a slightly surprising instant/&quot;local&quot; effect from `update`.

If you want the contract with the CRDT programmer to be more explicit, you can simply remove `update` from the API and force developers to invoke `merge` locally. This is nearly the same as the above design, except for type signatures:
- `merge`$: S \to S$
- `update`$: U \to (S \to S)$ 

That is, `update` can take values from some other domain $U$ and map them into mutations to $S$. Removing the `update` API  requires programmers to do that mapping at application level, but is more transparent about a semantics that &quot;`merge` always happens immediately to ensure inflationarity&quot;.


---

# Additional Technical Material
Just in case the above wasn&apos;t enough for you, here are a few more points that may be of interest to aficionados!

## &lt;a id=&quot;op-based&quot;&gt; &lt;/a&gt; Brief Note: Op-Based CRDTs are Deterministic

In [a previous post](../crdt-turtles#op-based) I explained how a fully-specified Op-Based CRDT is a join semi-lattice of a particular kind: a grow-only set lattice over the powerset $P(C \times O)$ containing tuples $(c_i, o_i)$, each comprised of a **causal context lattice** of type $C$ and a value from a domain of mutually commutative operations $O$. The &quot;causal context&quot; is typically a vector clock or DAG that dictates which of the other items in the set precede this one in a partial order.

At a gloss, the `update` function of an op-based CRDT takes as input an op $o_i \in O$ and does the following:

1. increment the local causal context to $c_{t+1}$ (e.g. a new vector clock value with the local nodeId&apos;s value incremented) 
2. add a tuple $(c_{t+1}, o_i)$ to the local set

This is clearly inflationary. From the semi-lattice perspective, we are simply adding an element to a set, so the result of update is a &quot;bigger&quot; set. Similarly, the underlying causal history is growing: 1 node is added, some precedences are added to the causal context, and nothing is taken away.

There are some nuances around &quot;expiring&quot; history from the partial order of ops that I discuss in that same post; they do not change the inflationary nature of Op-Based CRDTs.

&lt;Callout type=&quot;info&quot;&gt;
**Op-Based CRDTs are deterministic**, because their updates are inflationary by definition.
&lt;/Callout&gt;

---
## What About Monotonicity?
If you know of me from the CALM Conjecture, you might be suprised that I haven&apos;t used the &quot;M&quot; word yet: **monotonicity**. In fact, many authors in this space‚Äîmyself included‚Äîhave been imprecise in when we use monotonicity vs inflationarity. Let me clear this up here in this context.

We should start with crisp definitions.

### Monotonicity

$$
x \leq y \quad \Rightarrow \quad f(x) \leq f(y)
$$

Monotonicity is a **relational property**: it relates how the ordering on inputs maps to the ordering on outputs. At a gloss, it ensures that the more you put into the input, the more you get in the output.

### Inflationarity

$$
x \leq f(x)
$$

Inflationarity is a **pointwise property**: it compares each input directly to its own output. It ensures that applying the function doesn&apos;t discard information from its input.

### So What?
Well first off, you should know that these two properties are orthogonal: there are functions that are neither, either, or both.

Going back to our $\text{DropTop}$ function. Recall the definition:

$$
\text{DropTop}(x) = \begin{cases}
  a &amp; \text{if } x = \top \\
  x &amp; \text{otherwise}
\end{cases}
$$

This is clearly non-inflationary, assuming $a \ne \top$.

&lt;div className=&quot;diagram-float&quot;&gt;
    &lt;img src={withPrefix(&quot;/img/3-element.svg&quot;)} 
    alt=&quot;DropTop on a 3-element domain&quot; 
    style={{ display: &quot;block&quot;, maxHeight: &quot;120px&quot;}} 
    /&gt;
&lt;/div&gt;

Let&apos;s consider $\text{DropTop}$ over the domain $S = \{\bot, a, \top\}$ where $\bot \lt a \lt \top$. In this case, we have a non-inflationary function that *is* monotonic!  Let&apos;s work out the details:

- $\bot \leq a$ and $\text{DropTop}(\bot) \leq \text{DropTop}(a)$
- $a \leq \top$ and $\text{DropTop}(a)\leq \text{DropTop}(\top)$, because $\text{DropTop}(a) = \text{DropTop}(\top) = a$.


We&apos;ve seen above that $\text{DropTop}$ leads to non-determinism over the totally ordered domain *even though* it is monotonic. 

But here&apos;s a funny thing: consider the same function over the domain $S = \{\bot, a, b, \top\}$, where the join generates a partial order $\bot \leq \{a, b\} \leq \top$:
&lt;div className=&quot;diagram-float&quot;&gt;
    &lt;img src={withPrefix(&quot;/img/4-element.svg&quot;)} 
    alt=&quot;DropTop on a 4-element domain&quot; 
    style={{ display: &quot;block&quot;, maxHeight: &quot;120px&quot;}} 
    /&gt;
&lt;/div&gt;

In this domain, $\text{DropTop}$ is **non-monotonic!**.  To see it, consider that:

- $ b \leq \top$, but $\text{DropTop}(b) \not\leq \text{DropTop}(\top)$ because $\text{DropTop}(\top) = a$, and $a$ and $b$  are incomparable!

The same scenario we used to show that $\text{DropTop}$ leads to non-determinism in the previous totally-ordered domain shows it leads to non-determinism over this partially ordered domain, where it *is non-monotonic*. 

So: one monotonic example, one non-monotonic example. Neither is inflationary, and both lead to non-deterministic outcomes!  The relevant issue in this situation should now be clear: **updates need to be inflationary to achieve deterministic outcomes; monotonicity of update functions is irrelevant to determinism**.

### How Does this Relate to the CALM Theorem?
Great question! First, as a member of the University of California faculty, I&apos;d be happy to change the name to the CALI Theorem if inflation were the key property, rather than monotonicity!

But we should stay CALM.

The &quot;type signatures&quot; of these two terms give us a hint as to when we use them. Inflationarity is a property that only makes sense on functions from a domain to itself (*endofunctions*, if you like fancy math terms). Monotonicity is a property that we can define without regard for the domain and range of a function.

CRDTs are scoped as abstract data types, and hence mostly just capture transformations from state to state within a single domain. Hence inflationarity is an appropriate lens for them.

The CALM Theorem reasons about entire programs, and as such it cannot be defined in terms of inflationarity alone. Usually we want to talk about inflation of inputs to the program over time, and monotonicity of the logic that the program applies to those inputs to produce outputs that may well be in another domain. This is not a new observation; this distinction was made carefully by Ameloot, Neven and van Den Bussche in their [first proof of the CALM Theorem](https://dl.acm.org/doi/10.1145/1989284.1989321).

Returning to CRDTs, it&apos;s easy to show that **semi-lattice join is monotonic** in both inputs. We can also observe that *CRDTs are both inflationary and monotonic in their final, converged outcomes*: if you run a CRDT to convergence on some set of updates $U_1$, and then run it again to convergence on a set of updates $U_2 \geq U_1$, you&apos;ll find the the outcome of the second run is no lower in the lattice than the outcome of the first run‚Äîeven if your update functions are non-inflationary!

---

## Want More?
This is not the first writing that distinguishes between inflationary and monotonic functions in the context of distributed or parallel computing.

Here are a couple interesting and relevant prior references I&apos;ve found. I&apos;d love to know of others!

[LVars](https://dl.acm.org/doi/10.1145/2502323.2502326) inventor **Lindsey Kuper** has a 2015 blog post, *[What‚Äôs the difference between inflationary and monotonic functions?](https://decomposition.al/blog/2015/08/31/whats-the-difference-between-inflationary-and-monotonic-functions/)* that highlights the confusion many have regarding inflationarity and monotonicity, and gives intuitive examples to demonstrate that they are orthogonal properties.

**Almeida et al.** recently published a CRDT survey (*[Computing Surveys, 2023](https://dl.acm.org/doi/10.1145/3695249)*) that includes a discussion of **inflationary** and **monotonic** updates, and argues that prior work erroneously demanded monotonic update functions, when it should have demanded inflationary functions. In fact, as we discuss above, the SEC guarantee of CRDTs does not demand inflationarity or monotonicity of update! Inflationarity is only necessary if we want determinism or lower bounds (monotonicity of `read`). Moreover, their example (page 18) of a monotonic but non-inflationary function is *decrement*, which is a bit confusing: decrement can be inflationary in some contexts (e.g. a countdown timer based on natural numbers with `min` as the `merge` function), but anti-inflationary (&quot;deflationary?&quot;) in other contexts (e.g. a typical integer/`max` counter semi-lattice.) It&apos;s nicer to have an example like $\text{DropTop}$ that is neither inflationary nor anti-inflationary, and which separates inflationarity and monotonicity directly (by simply expanding the domain).

I am not aware of prior CRDT literature that explicitly discusses the idea that inflationarity is a requirement for deterministic convergence under SEC. If you know of such work, please link to it in a comment below, or reach out if you don&apos;t have a github account!

&gt; Thanks to [Conor Power](https://www.linkedin.com/in/conorpower23/) for sparking my interest in this topic and providing sanity checks on early drafts of this post.
</content:encoded></item><item><title><![CDATA[CRDTs #3: Do Not Read!]]></title><description><![CDATA[Ever used a CRDT, thought you were safe, and‚Äîboom‚Äîyou bought a Ferrari you didn't mean to? It could happen to you! The truth is that CRDTs‚Ä¶]]></description><link>https://jhellerstein.github.io/blog/crdt-dont-read/</link><guid isPermaLink="false">https://jhellerstein.github.io/blog/crdt-dont-read/</guid><pubDate>Fri, 23 May 2025 00:00:00 GMT</pubDate><content:encoded>
import Callout from &apos;../../../src/components/Callout&apos;
import { withPrefix } from &quot;gatsby&quot;

Ever used a CRDT, thought you were safe, and‚Äîboom‚Äîyou bought a Ferrari you didn&apos;t mean to? It could happen to you!

The truth is that CRDTs are dangerous to observe: they 
guarantee *eventual consistency*, but you&apos;ll never know when &quot;eventual&quot; arrives.
That gap between what CRDTs promise and what you can safely `read` is generally not 
well protected in CRDT libraries, and it&apos;s exactly where the bugs sneak in.
**It is not safe to `read` a CRDT&apos;s raw state, people!**

What CRDTs *can* offer‚Äî*when used properly!*‚Äîis **monotonicity**:
the guarantee that once you&apos;ve seen a fact, no future update will contradict it. 
And that&apos;s a powerful basis for providing safe APIs to your CRDTs.

&gt; This is the 3rd post in a [series](../crdt-intro/) I&apos;m doing on CRDTs. This one is particularly for you software engineers out there.
&gt; News you can use.

---

## Takeaways

&lt;Callout type=&quot;danger&quot;&gt;
**Look not at the naked state of thy CRDT!** Encapsulate it, and break that encapsulation 
cautiously ...with plenty of code review and comments!
&lt;/Callout&gt;

&lt;Callout type=&quot;warning&quot;&gt;
**You will never experience eventuality**. Eventual consistency is an abstract concept, not a guarantee you can count on.
&lt;/Callout&gt;

&lt;Callout type=&quot;success&quot;&gt;
**All is not lost!**
The monotonicity of many CRDTs can help, especially via [threshold functions](#threshold-functions).

In general, you&apos;ll need coordination to know when you&apos;re done‚Äîuse it, sparingly and strategically.
&lt;/Callout&gt;

&lt;Callout type=&quot;info&quot;&gt;
See the [CRDT Survival Guide](#survival-guide) at the end of the post!
&lt;/Callout&gt;
---
## Prologue: ‚ÄüEventual Consistency‚Äù
Have you ever asked that age-old question, &quot;Are we there yet?&quot; Eventual consistency promises you&apos;ll eventually get to an agreed-upon value across replicas... but when is &quot;eventual&quot;? Tomorrow? Next year? After you&apos;ve bought a Ferrari you didn&apos;t mean to? 

Werner Vogels [defined eventual consistency like this](https://dl.acm.org/doi/10.1145/1435417.1435432):  
&gt; *If no new updates are made to the object, eventually all accesses will return the last updated value.*

Sounds reassuring, right? And it avoids expensive coordination protocols like Paxos or Two-Phase Commit. 

But here&apos;s the catch: *How do you know there are no new updates?*
In distributed systems, termination detection (i.e. &quot;am I done yet?&quot;) requires knowing:

1. **No node** will issue any new messages.
2. **No messages** are in flight.

In logic, that&apos;s:

$$
\neg\exists n  \; (p(n))
$$

&quot;there does not exist an $n$ where property $p(n)$ holds&quot;. 

Any time you see a $\neg\exists$ (or its doppelganger, $\forall$) in distributed logic, *beware!* One rogue message ‚Äî *a single counter-example* ‚Äî can arrive at any time to invalidate the property.

üëâ **Termination (&quot;eventuality&quot;) is non-monotonic**. It can be true over a certain set of information, but become false if more data arrives.

The [CALM Theorem](https://cacm.acm.org/research/keeping-calm/) says that eventual consistency without coordination is possible *if and only if* the program specification is monotonic. Thus (via CALM):

üëâ **Termination Detection requires Coordination**. 

So ... in coordination-free systems, you can never know when &quot;eventual&quot; has arrived!

&lt;details&gt;
&lt;summary&gt;Click for a review of monotonicity.&lt;/summary&gt;

Given a function $f:S \rightarrow T$, where $S$ and $T$ are ordered domains, we say that $f$ is **monotone** (or **monotonic**) if:

$$
x \le y \implies f(x) \le f(y)
$$

Intuitively, a monotone function *preserves order*: it guarantees that if the input gets &quot;bigger&quot;, then the output gets no smaller.

Monotonicity is often used in logic, where our domains $S$ and $T$ contain sets of facts. Given an input set $x$, a logical function $f$ produces a set of *conclusions*, $f(x)$. If $f$ is monotone, $x \subseteq y \implies f(x) \subseteq f(y)$: that is, $f(y)$ contains all the facts in $f(x)$ and perhaps more. 

In practical terms, if we think of $f$ as a process running over a growing stream of facts, we can say this: *once an output fact is concluded by a monotone function, additional input facts will not invalidate that conclusion*.

You can see why this is useful in a distributed system! 

1. Monotone functions allow for correct, wait-free, streaming computation.
2. For logical monotone functions, the truth of each conclusion is invariant in the face of additional input.
&lt;/details&gt;


---

## üôãüèæ‚Äç‚ôÇÔ∏è Is Anything Safe Before ‚ÄüEventual&quot; comes?

&lt;Callout type=&quot;danger&quot;&gt;
In general, no! The state of a CRDT could be incomplete and may change.
&lt;/Callout&gt;

This is a common misunderstanding. It&apos;s easy to confuse the formal guarantees of `merge` (which CRDTs provide) with the safety of `read`s (which they absolutely do not). 

If you previously missed this, you are in good company. The danger of `read` is not prominent in online discussion, software packages, or the CRDT literature. Perhaps this post can serve as a warning and a pointer to more subtle discussion in the literature.

Let&apos;s illustrate the problem‚Äîand ways to use CRDTs responsibly.

---

### üö® Two-Phase Sets: Poster Child of the Problem

A 2-Phase (Add/Remove) Set CRDT maintains two sets:

```rust
struct 2PSet {
    adds: Set&lt;(id, element)&gt;;
    removes: Set&lt;(id, element)&gt;;
}
```

Merges? Safe‚Äîjust union both sets.

```rust
fn merge(a: &amp;2PSet, b: &amp;2PSet) -&gt; 2PSet {
  2PSet {
    adds: a.adds.union(&amp;b.adds),
    removes: a.removes.union(&amp;b.removes),
  }
}
```

But the `read`?

```rust
fn read(s: &amp;2PSet) -&gt; Set&lt;element&gt; {
  s.adds - s.removes
}
```

This `read` is **non-monotonic**: as `removes` grows, your `read` result *shrinks*. That&apos;s the trap.

So what could go wrong?

#### ü•î üèéÔ∏è The Potato/Ferrari Example
&lt;div className=&quot;diagram-float&quot;&gt;
  &lt;img src={withPrefix(&quot;/img/ferrari-diagonal.svg&quot;)} alt=&quot;CRDT Ferrari Sequence Diagram&quot; style={{ width: &quot;85%&quot;, display: &quot;block&quot; }} /&gt;
&lt;/div&gt;
Here&apos;s an example of what could go wrong, from our [Keep CALM and CRDT On](https://www.vldb.org/pvldb/vol16/p856-power.pdf)  paper:


&gt; While shopping online at RetailCo, you add a potato and a Ferrari to your cart. Reflecting on your finances, you decide to remove the Ferrari, and check out.&lt;br/&gt;

```rust
cart.add(&quot;potato&quot;);
cart.add(&quot;Ferrari&quot;);
cart.remove(&quot;Ferrari&quot;);
cart.checkout();
```

The CRDT-based cart is replicated, and unfortunately replica `B` receives `checkout` before `remove`. It `read`s the cart, and ships you a Ferrari. Boom. &lt;br/&gt;


üëâ **Merges are safe; reads are not.** 2P-Sets? Nearly useless for safe reads.
### What about Simpler CRDTs?
You might be saying:

*Well, 2P-sets use a *set difference* operator, which is clearly non-monotonic. The CALM Theorem warned us that non-monotonic operations require coordination for consistency! 
But surely a plain old grow-only set is safe to read? After all, its `read` function looks nice and monotonic!*

&lt;div className=&quot;diagram-float&quot;&gt;
  &lt;img src={withPrefix(&quot;/img/ingredients.svg&quot;)} alt=&quot;CRDT Ingredient Sequence Diagram&quot; style={{ width: &quot;100%&quot;, display: &quot;block&quot; }} /&gt;
&lt;/div&gt;

You&apos;d be right that the `read` is monotonic:
```rust
read(s: GrowOnlySet) -&gt; Set {
  s.adds
}
...
let c = GrowOnlySet.new();
```

And that seems safe locally. But wait until you see the downstream logic:

```rust 
let ingredients = c.read();
if edible(&amp;ingredients) {
  cook(&amp;ingredients);
} else {
  panic!(&quot;InedibleError&quot;);
}
```

Both replicas start out empty‚ÄîI think we can all agree that the empty set is inedible. 
But imagine replica `A` merges some yummy stuff:
```rust
c.merge([&apos;apples&apos;, &apos;honey&apos;]);
```
We transition from `!edible` to `edible`.

Now suppose replica `B` merges some more stuff:
```rust
c.merge({&apos;bleach&apos;, &apos;Paxos&apos;})
```
Bleach and Paxos both have their uses, but please dont ingest them!  Merging in more stuff transitions replica `A` back from `edible` to `!edible`.
Even though `c` grows monotonically, `edible` is not a monotone function over `c`,
so the result of `edible(c)` toggles from false to true to false at node `A`!

üëâ **The general point: even monotonic `read`s can lead to non-monotonic conclusions in downstream code.**

This seems like pretty bad news! But all is not lost. Let&apos;s look at some ways to work with CRDTs that provides
some guard rails.

---
## &lt;a id=&quot;be-safe&quot;&gt;&lt;/a&gt; ü¶∫ Safety First: Encapsulate CRDT State
To prevent the kinds of surprises we just saw, CRDT state should be *encapsulated*, using a language that supports strong typing. If `read` is offered, it should be marked as `unsafe`.

A compiler *might* allow `read` without `unsafe` *if* it can prove all downstream logic is monotonic. But that&apos;s rare. Monotonicity is undecidable in general.

If you&apos;re in Rust, check out [Hydro](https://hydro.run): we&apos;re working on these issues!


## üëç Safe, Practical CRDT Usage: Lower Bounds and Threshold Functions

Are there any functions that can safely examine CRDT state? Yes indeed! Monotonicity to the rescue.

Specifically, since the value of a properly-written CRDT should only go up over time, CRDTs give you trustworthy *lower bounds*. Just don&apos;t treat a lower bound as a final answer‚Äîa lower bound is a special type,
which you can only compare using `&lt;=`! In particular, you can&apos;t test for equality (`==`) with a lower bound. 

In addition, *you can expose monotone functions on CRDTs* to safely compute on their evolving state. 
Let&apos;s see how.


### &lt;a id=&quot;threshold-functions&quot;&gt;&lt;/a&gt; ‚úÖ Thresholds: Coordination-Free Termination

Some lattices are bounded, which means they have a unique top element ($\top$). Once you hit $\top$, you&apos;re done! As a classic example, consider the boolean lattice with values `{false , true}` and merge function that computes $\vee$ (logical `or`).

**Threshold functions** are boolean functions (i.e. truth predicates) on lattices that exploit this:
- They map from a big (or unbounded) lattice to the boolean lattice
- They are *monotone* functions: as the input gets bigger, the output can never go down -- once `true`, always `true`!
- `true` is $\top$ and *safe to `read`*

Clearly `edible` is not a threshold function. What is a good example? Here are two examples on grow-only set lattices: once true, always true!

```rust
state.len() &gt; 100;
state.contains(&apos;Apple Sauce&apos;);
```

CRDTs and threshold functions can be pretty useful. Even if your full lattice (like a set) has no practical $\top$, your threshold function does! Once you cross that threshold, you can treat the truth value as a stable boolean value‚Äîone that will be eventually consistent across nodes. So you can `read` the output of the threshold function safely.

Threshold functions are a common example, but you can safely use any monotone function that maps to any finite, 
ordered type!
But remember: *until* your monotone function hits $\top$ in your output type, you&apos;re still in unsafe territory. 
`Read`s may still change! So threshold functions are only helpful when they become true $(\top)$.

---

## üß≠ So What Should Systems Do?
Realistically, many eventually consistent systems need to use some coordination at some point. And in many cases that&apos;s OK, especially if we can *avoid coordination most of the time*! As my colleague [Natacha Crooks](https://nacrooks.github.io/) said once, &quot;most programs are not monotonic, but most programs are mostly monotonic&quot;. So the trick is to put coordination in its place. 

Here&apos;s some advice as you think about eventual consistency, CRDTs, monotonic programming, and the like:

**1. Coordination is still needed to *know* when you&apos;re done.**  
Use it sparingly! For example, when you&apos;re pretty sure every node is done with a task or session ‚Äî maybe because some coordination-free threshold has been met ‚Äî you can employ a round of consensus to detect termination. (Of course if it fails you may have to wait and try again later.)

**2. Don&apos;t trust CRDTs that have non-monotonic `read`s.**  
Non-monotonic `read` methods like that of 2P-sets are *unsafe in any context:* it doesn&apos;t matter what you do downstream, the `read` itself exposes you to non-monotonicity and hence race conditions. 2P-sets and their more complicated sibling, OR-sets, are quite troublesome in that respect.
The only safe way to use them is to do coordination for each `read`‚Äîwhich probably makes 2P-sets no more efficient
than your favorite transactional database!

**3. Embrace strong typing and escape hatches.**
CRDT state should be encapsulated, and methods that expose the state should be marked `unsafe`. Even if the `read` is monotonic, downstream logic may not be. 
There are certainly cases where developers will want to take their non-deterministic chances `read`ing the 
state of a CRDT, and that&apos;s their business! But for purposes of maintainability and code review, risky behavior of that sort should be explicitly flagged in code, just like Rust requires us to flag unsafe memory accesses.

**4. Monotonic thresholds are your friend.**  
Thresholds and other monotone functions enable safe, observable progress without coordination ‚Äî *if* you expect to hit $\top$.

In summary, I offer this:
### &lt;a id=&quot;survival-guide&quot;&gt;CRDT Survival Guide&lt;/a&gt;

&lt;Callout type=&quot;success&quot;&gt;
**Safe:** `merge` freely, take advantage of threshold functions.
&lt;/Callout&gt;

&lt;Callout type=&quot;warning&quot;&gt;
**Unsafe:** `read` at your own risk.
&lt;/Callout&gt;

&lt;Callout type=&quot;danger&quot;&gt;
**Avoid:** Non-monotonic reads like in 2P-sets.
&lt;/Callout&gt;

&lt;Callout type=&quot;info&quot;&gt;
**Pro Tip:** Treat CRDT state like a radioactive material‚Äîencapsulate it, mark `read` as unsafe.
&lt;/Callout&gt;

---

## üß† Want More?

If you&apos;re looking for formal research in this space that goes beyond the main CALM Theorem papers, check out these more recent results:

- Conor Power&apos;s recent theoretical work on [Free Termination in ICDT 25](https://drops.dagstuhl.de/storage/00lipics/lipics-vol328-icdt2025/LIPIcs.ICDT.2025.32/LIPIcs.ICDT.2025.32.pdf) goes beyond thresholds 
to identify more cases where you can terminate without coordination. 
- Be aware that researchers have found extensions to the CALM theorem, where global knowledge can allow coordination-free computation in more cases.
The most recent paper in this line of work is from our friends Tim Baccaert and Bas Ketsman in a [PODS 2023](https://dl.acm.org/doi/10.1145/3584372.3588657) paper.
- For original research on threshold functions, see Kuper and Newton&apos;s [LVars](https://dl.acm.org/doi/10.1145/2502323.2502326).

...and stay tuned for the next post on CRDTs&apos; algebraic properties.
</content:encoded></item><item><title><![CDATA[CRDTs #2: Turtles All the Way Down]]></title><link>https://jhellerstein.github.io/blog/crdt-turtles/</link><guid isPermaLink="false">https://jhellerstein.github.io/blog/crdt-turtles/</guid><pubDate>Thu, 22 May 2025 00:00:00 GMT</pubDate><content:encoded>&lt;blockquote class=&quot;quote&quot;&gt;
&lt;p&gt;
After a lecture on cosmology, William James was challenged by a skeptic:  
&lt;/p&gt;

&lt;p&gt;
&quot;Your theories are incorrect. The Earth rests on a turtle,&quot;&lt;br /&gt;
&quot;And what holds up the turtle?&quot; James asked.  &lt;br /&gt;
&quot;Another turtle,&quot; came the reply. &lt;br /&gt;
&quot;And what holds that up?&quot; pressed James.
&lt;/p&gt;

&lt;p&gt;
The skeptic was undeterred:&lt;br /&gt;
&quot;You can&apos;t fool me, sir. It&apos;s turtles all the way down.&quot;
&lt;/p&gt;

&lt;p&gt;
&lt;em&gt;‚Äî Anecdote attributed to William James ([via J.R. Ross, 1967](https://en.wikipedia.org/wiki/Turtles_all_the_way_down))&lt;/em&gt;
&lt;/p&gt;
&lt;/blockquote&gt;

*This is the 2nd post in a series of 4 posts I&apos;m doing on CRDTs. Please see the [intro post](../crdt-intro/) for context.*

Modern distributed systems often seem to rest on an stack of turtles.
For every guarantee we make, we seem to rely on a lower-layer assumption. Eventually we&apos;re left wondering: what *is* at the bottom?

CRDTs ‚Äî *Conflict-Free Replicated Data Types* ‚Äî are often advertised as a foundation we can finally trust.
They promise convergence of state across machines *without* requiring perfect clocks, global operation ordering, or causal message delivery ... and they do it with math.

But many CRDTs sneak in assumptions that don&apos;t belong. That&apos;s not solid ground. It&apos;s not math. It&apos;s turtles.

In this post, we‚Äôll show how to design CRDT internals properly:

- ‚úÖ Always in terms of a semilattice structure.  

- ‚úÖ Always with clean algebraic reasoning, without hidden dependencies. 

- ‚úÖ With explicit causality lattices included whenever needed. 


This will ensure we&apos;re always using careful reasoning.

*Correct CRDTs are semilattices at bottom.* And that&apos;s math you can count on.

## üê¢ A Principle for CRDTs: Semilattices All the Way Down
Every well-designed CRDT is a **semilattice**.

- ‚úÖ A semilattice defines how information grows and `merge`s.
- ‚úÖ It provides convergence by construction, through clean algebra.

In case you&apos;ve read about a split between so-called &quot;state-based&quot; vs &quot;op-based&quot; CRDTs, you can ignore that for now; it&apos;s a turtlish distraction I will [fill in below](#op-based). Here‚Äôs what actually matters:

&gt; A semilattice is:
&gt; - A set of states $S$
&gt; - A `join` function $\sqcup : S \times S \to S$ that must satisfy **commutativity**, **associativity**, and **idempotence**. 
&gt; The `join` function induces a partial order:
&gt; $x \leq y \iff x \sqcup y = y$.

When discussing CRDTs, people often use the term `merge` instead of `join`.


CRDTs sometimes add additional &quot;update&quot; operators: 
&gt; `update`$: U \to (S \to S)$ 
`update` takes an input value of type $U$ and uses it to directly mutate the local CRDT&apos;s state.

If all pairs of nodes eventually `merge` state in an associative, commutative and idempotent manner, then eventual convergence of a CRDT is guaranteed ‚Äî no further assumptions required.

## üîç Common CRDT Mistake: Hiding Assumptions
Many CRDT descriptions assume causal message delivery, message uniqueness, or reliable clocks ... but fail to encode these in their semilattices.

üö´ That‚Äôs like putting turtles back under the CRDT again!

### ‚úîÔ∏è Design Rule:
&gt; All required assumptions must be **internalized** in the semilattice structure.

- If your algorithm needs causality, encode it.

- If it expires or compresses away state, model that algebraically too, and make sure it respects the rules of a semilattice.

- You can always optimize later (see [below](#building-on-an-existing-turtle)) ... but the math must be sound on its own.

## Case Study: Add/Remove Sets
Let&apos;s walk through a concrete example. A 2-Phase (2P) Set is a simple CRDT that tracks a pair of set-based lattices `(adds, removes)` where `merge` is set-union for each:
- **adds**: `{(id, element)}`
- **removes**: `{(id, timestamp)}` (sometimes referred to as **tombstones**)

The 2P-Set is a **free product** of these two set lattices, which is to say that the 2P-Set `merge` operator  is simply the independent `merge` of 2 **adds** sets, and 2 **removes** sets:

$$
(a_1, r_1) \sqcup (a_2, r_2) =
    (a_1 \sqcup a_2, b_1 \sqcup b_2)
$$



Updates are simple: add an item by inserting into **adds**, delete an item by placing its id and time of deletion into **removes**. All good.

Until... you try to expire tombstoned data to save space.

### Observed-Remove (OR) Sets

The OR-Set CRDT extends 2P-Sets to allow tombstones to be expired, but ... it&apos;s tricky! Let&apos;s walk through it.

A naive scheme for expiring tombstones might work as follows: look at a local wall-clock, and expire ids from **adds** and **removes** whose tombstone timestamps are &quot;older&quot; than a threshold. Turns out that this would be bad! Making this decision based on local time can cause **non-convergent** behavior. 

This is not at all obvious (in fact, ChatGPT happily provided incorrect proofs in both directions!), so I constructed a proof by example.  The basic idea is this: even after all updates have been issued, nodes can pass an item back and forth as a &quot;hot potato&quot; indefinitely, and never converge despite communicating infinitely often! 

&lt;details&gt;
&lt;summary&gt;Click to see a non-convergent OR-Set cycle infinitely.&lt;/summary&gt;
&lt;a href=&quot;../img/divergence_fsm_piechart.png&quot;&gt;
  &lt;img
    src=&quot;../img/divergence_fsm_piechart.png&quot;
    alt=&quot;FSM Divergence Diagram&quot;
  /&gt;
&lt;/a&gt;
&lt;p&gt;
  This diagram shows an oscillating state change cycle -- a single item in an OR-set that uses naive local expiry and never converges, just keeps rotating from state to state forever. Each &apos;pie&apos; represents a &lt;em&gt;global&lt;/em&gt; state of the item, across each of three nodes, &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt;. In each state, each of the machines either has the item only in the adds set (&lt;code&gt;+&lt;/code&gt;), in the adds and removes sets (&lt;code&gt;‚Äî&lt;/code&gt;) or in neither (&lt;code&gt;?&lt;/code&gt;). Edges are labeled with state transitions: &lt;code&gt;xp@A&lt;/code&gt; means that the item expired at node &lt;code&gt;A&lt;/code&gt;; &lt;code&gt;B &amp;lt;- A&lt;/code&gt; means that node &lt;code&gt;B&lt;/code&gt; received a copy of the item from node &lt;code&gt;A&lt;/code&gt;.
&lt;/p&gt;
&lt;p&gt;
  Click on the image to zoom if needed.
&lt;/p&gt;
&lt;/details&gt;

#### üßØ Fix: Explicit Causality
This brings us back to the main point of this post: we need to *explicitly* include information in our OR-set semilattice ... in this case, to support convergent expiry of state. Specifically we can use a nested semilattice to track a **causal context**‚Äîe.g. a **version vector**‚Äîand use that to determine when it&apos;s safe to expire items:

- ‚úÖ Expire a tombstone only after every node is guaranteed to know about it.

Note that this constraint breaks the cycle in the diagram of non-convergence above! It forbids the edges `S2 -&gt; S3`, `S5 -&gt; S6` and `S8 -&gt; S0`: each of those edges represents a tombstone being expired when at least one node is in a green `+` state and doesn&apos;t believe the tombstone exists!

We enforce the constraint by making the OR-Set semilattice a [lexical product](https://en.wikipedia.org/wiki/Lexicographic_order) semilattice: 

```
(causalContext, (adds, removes))
```

Unlike our previous *free product*, the `merge` operator for the lexical product only looks at its second field `(adds, removes)` when breaking ties on the first field `causalContext`:

$$
(cC_1, (a_1, r_1)) \sqcup (cC_2, (a_2, r_2)) =
\begin{cases}
  (cC_1, (a_1, r_1)) &amp; \text{if } cC_1 &gt; cC_2 \\\
  (cC_2, (a_2, r_2)) &amp; \text{if } cC_2 &gt; CC_1 \\\
    (cC_1 \sqcup cC_2, (a_1 \sqcup a_2, b_1 \sqcup b_2)) &amp; otherwise
\end{cases}
$$


Note that the `causalContext` is itself another semilattice! It tracks which operations have been observed system-wide. This tracking can be stale, but it is always a conservative lower bound. We can safely expire data from our OR set if it is older than our `causalContext`.

There are different implementations for `causalContext`, including *version vectors* or *causal graphs*. We&apos;ll work with version vectors since they&apos;re the most common.

&lt;details&gt;
&lt;summary&gt;Click to learn about version vectors.&lt;/summary&gt;

We begin by ensuring that each node maintains a *local clock* -- a counter that increments by 1 each time the node applies an operation or sends a message. (Note that a counter is also a semilattice, where the domain $S = \mathbb{N}$ is the natural numbers 0, 1, 2, ..., and the `merge` function is `max`.)
&lt;br /&gt;
&lt;br /&gt;

A *version vector* is a map from `nodeId`s to values from a counter lattice: it records the highest clock value a node has heard of *from each other node*. This map is itself a composite semilattice! Specifically:

- The domain $S$ is a map from `nodeId` (the key) to a value from the lattice $(\mathbb{N},$ `max`$)$ (the value)
- The `merge` function is simply key-wise application of the value lattice `merge` function (i.e., `max`). If a key is missing from one input to `merge`, we simply take its value from the other input.
&lt;/details&gt;

Notice what we did here: we formed a *composite* semilattice `(causalContext, (adds, removes))` out of very simple semilattice building blocks.
The `merge` functions of these lattices effectively invoke the encapsulated sub-lattice `merge` functions recursively.

*It really is lattices all the way down!*


#### Using Version Vectors for Safe Expiration
To use our version vectors, we will make a few small changes to our OR-set design:

1. Each node locally maintains an overall version vector containing the `merge` of *all* version vectors seen so far: this is typically called a *vector clock*. It represents a high-watermark of our local knowledge of global progress. 
2. When an item is deleted, its tombstone timestamp is set to the local vector clock.

We can now do expiration safely: tombstones are only expired if their timestamp is lower in the partial order than the local *vector clock*: if so, we can be sure that *every other node also knows about this tombstone, and will eventually expire it as well*.

## &lt;a id=&quot;op-based&quot;&gt;&lt;/a&gt;A Note on Op-Based CRDTS
As mentioned above, many CRDT fans like to talk about two &quot;different&quot; kinds of CRDTs: normal (&quot;state-based&quot;) semilattice CRDTs, and something called &quot;op-based&quot; CRDTs. *I&apos;m here to tell you that correct op-based CRDTs are also semilattices; the distinction is not fundamental.*

An &quot;op-based&quot; CRDT is just a particular class of semilattice. The state of an op-based CRDT represents a *partially-ordered log of operations* (opaque commands). The CRDT&apos;s job is to ensure that the partially-ordered log is consistent across nodes. 

The partial order among ops can be captured by each site tagging every new op it generates with a `causalContext` value. This ensures (1) that recipients of ops from node $n$ will have them ordered in the same way as $n$ did, and (2) operations *across* nodes are causally ordered, via the `causalContext`.

Specifically, the state $S$ of an op-based CRDT can simply be a *set* of `(causalContext, op)` tuples, with simple set-union as the `merge` function. The `causalContext` is ignored by the lattice `merge`, but carried along to preserve a consistent partial order of the log. One typical `causalContext` implementation is to use vector clock timestamps, with each node incrementing its entry in the vector clock for every op and message.

That&apos;s really all there is to an &quot;op-based&quot; CRDT: it&apos;s a grow-only set of causally-stamped commands. 

Typically, op-based CRDT designs assume that the log at each site is &quot;played&quot; (eagerly or lazily), by executing the ops in their causal partial order to materialize the local state. This is only required to support a &quot;read&quot; operation, and hence is effectively outside the scope of the CRDT math. Because causal order is only a partial order, different nodes could &quot;play&quot; some ops in different orders. As a result, op-based CRDT designs typically require the ops themselves to be mutually commutative. 

If an op-based CRDT has quiesced and propagated to every node, and the ops themselves are mutually commutative, then every node can &quot;play&quot; the log in some total order that respects the partial order, and all nodes will end up with a convergent outcome. 

To summarize: an op-based CRDT is still just a simple set semilattice! The only wrinkles are:
1. The items in the op-based CRDT set are stamped with causalContext to enable causally-ordered replay
2. For the ops to be meaningful at replay time, ops across sites should be commutative.

## ü™ú &lt;a id=&quot;building-on-an-existing-turtle&quot;&gt;&lt;/a&gt;You Can Build on a Turtle ‚Äî But Know What It Carries
Sometimes, a system&apos;s lower layers provide additional guarantees that allow us to skip some details and rely on a turtle below us.

&gt; Example: If your network guarantees causal delivery, you can safely drop explicit causal tracking in your CRDT.

But beware: your CRDT is now resting on that turtle. If the network is not in fact behaving like a causal semilattice, your convergence proofs go out the window!

## üìå Takeaways
- ‚úÖ Every CRDT must be a (correct) semilattice
- ‚úÖ Order comparisons must respect the partial order induced by `merge`.
- ‚úÖ Model all necessary assumptions *inside* the lattice.
- ‚úÖ Build on trusted turtles only when you know exactly what they can carry safely.

When you do all that?
&gt; **It&apos;s semilattices all the way down**.

That&apos;s math you can build on.

</content:encoded></item><item><title><![CDATA[A Run of CRDT Posts]]></title><description><![CDATA[Over the next few days, I'm going to post a number of observations about CRDTs: ~~Convergent~~ Conflict-free Replicated Data Types. These‚Ä¶]]></description><link>https://jhellerstein.github.io/blog/crdt-intro/</link><guid isPermaLink="false">https://jhellerstein.github.io/blog/crdt-intro/</guid><pubDate>Wed, 21 May 2025 00:00:00 GMT</pubDate><content:encoded>Over the next few days, I&apos;m going to post a number of observations about *CRDTs*: ~~Convergent~~ Conflict-free Replicated Data Types. These are data structures that aspire to help us with *coordination-free distributed programming*, a topic that interests me a lot. How can developers (or languages/compilers) deliver distributed programs that are *safe* or *correct* in important ways, without employing expensive mechanisms for *coordination* that make the global cloud run as slowly as a sequential computer?

In a nutshell, my take is that CRDTs are built on an elegant kernel, but offer a leaky abstraction that misleads a lot of developers -- and researchers. Understanding the ideas and problems of CRDTs is a great way to walk into this domain. I&apos;ll give an overview in this post, and the series of posts will go futher.

## CRDTs: Pros &amp; Cons (Lattices &amp; Lettuces?)
First, the elegant part, which I find very appealing: 

1. **Deep Roots**: CRDTs are based on *semilattices*, which are simple, abstract mathematical structures that have a `join` operator that is *associative, commutative, and idempotent*. The idea to use this for replicated data types goes back at least to work by [Baquero and Moura in 1997](https://gsd.di.uminho.pt/members/cbm/ps/scadt3.pdf). They deserve more citations for this! (HT [Conor Power](https://www.linkedin.com/in/conorpower23) for educating me about this; I believe he learned about it from [Marc Shapiro](https://www.lip6.fr/actualite/personnes-fiche.php?ident=P1450) of CRDT fame.) 

2. **Moar Algebra!**: The use of modern algebra as a building block for correctness in distributed systems and database systems is a wonderful direction for the field, and we&apos;re seeing more and more of this work in recent years. (See for example the [Simons&apos; Institute gathering](https://simons.berkeley.edu/workshops/logic-algebra-query-evaluation#simons-tabs) from a couple years back.) This semilattice/CRDT line of work was early, elegant and easy to understand. Lovely stuff.

Unfortunately there are few key problems that arise in common discussion of CRDTs:

1. **Drifting from Correctness**. As people walk away from the semilattice foundation, they can lose their moorings in correct math. This is entirely avoidable, and most experts know how to avoid bugs here, but the discussion often gets unnecessarily subtle ... in ways that confuse people.

2. **Unsafe to Use**. The algebra of semilattices has a single operator: `join`. Notably it doesn&apos;t have any operator that corresponds to *read* or *inspect*. In fact, CRDTs as described in the literature provide *absolutely no guarantees to readers*, so a &quot;proper&quot; CRDT implementation should *not allow reads!* Which is to say a correct CRDT is an entirely useless theoretical object. Yet people use CRDTs, inevitably reading/inspecting them in unsafe/non-deterministic ways. Worse, many developers *think* they&apos;re getting useful correctness guarantees from CRDTs, which they are not! The only safe thing to do with a CRDT is to leave it unexamined.

3. **Programmability Issues**. As *unreadable data types*, CRDTs can&apos;t be composed safely into useful programs. How in fact can we use them?  Ideally we&apos;d like a *language* that allows correct composition of CRDT building blocks. This is something folks have looked at in DSLs like [LVars](https://dl.acm.org/doi/abs/10.1145/2502323.2502326), [Bloom^L](https://dl.acm.org/doi/abs/10.1145/2391229.2391230), [Lasp](https://dl.acm.org/doi/abs/10.1145/2790449.2790525) and [Gallifrey](https://par.nsf.gov/biblio/10095545). There&apos;s still work to do to deliver those ideas to developers in a familiar frame, which is one goal of our [Hydro](https://hydro.run) library for Rust.

### My Take
So ... it&apos;s true that I&apos;m not a huge fan of CRDTs as a practical matter. But I think the core ideas are quite lovely, and the pitfalls are interesting and really educational for developers and researchers to understand.  Much respect to the folks who&apos;ve worked on CRDTs over the years, both for what they&apos;ve invented and the challenges they&apos;ve raised.

I learned a lot unpacking my discomfort with CRDTs over the years with my students, so my next few posts will hopefully expose and summarize some of what we learned along the way. You can decide whether this makes you more or less likely to use CRDTs in your code, but hopefully your decisions and ensuing heuristics will be better informed.</content:encoded></item><item><title><![CDATA[Looking Back to Look Ahead]]></title><description><![CDATA[This is the second of two background posts reflecting on my technical interests, to set some context for this blog. While there's no‚Ä¶]]></description><link>https://jhellerstein.github.io/blog/looking-back/</link><guid isPermaLink="false">https://jhellerstein.github.io/blog/looking-back/</guid><pubDate>Wed, 30 Apr 2025 00:00:00 GMT</pubDate><content:encoded>
This is the second of two background posts reflecting on my technical interests, to set some context for this blog.
While there&apos;s no breaking news here, I hope that outlining my long-term threads of work might spark interest or recognition. If something resonates, I‚Äôd love to hear what caught your eye.

## Looking back on research to date

Thanks largely to collaborations, I&apos;ve worked on a lot of different projects over the years. At times, I‚Äôve felt a bit *too* scattered. Still, looking back, I see a through-line of long-term interests that have anchored my work and kept me engaged. These include formal programming models for distributed systems, the role of semantics in coordination, and the intersection of human insight and automated tools‚Äîthreads that continue to shape my work today.

### Formal languages for distributed programming

One of the signatures of the database field is its bold embrace of high-level declarative languages, and the multi-decade challenge of translating from there down to efficient execution. I&apos;m glad to be part of a scientific culture that is willing to be patient and dig deep! One of the themes I&apos;ve pursued over time has been to take the lessons of declarative query languages and try to adapt them to other domains. I&apos;ve been at this for over two decades, and I think we&apos;re now ready to deliver some big practical payoffs outside the research world.

This theme began with a run of work on [Declarative Networking](https://dl.acm.org/doi/10.1145/1592761.1592785) in the oughts. We started pursuing a broader focus on general-purpose distributed computing in the 2010&apos;s. Along the way I also worked on SQL-based machine learning in the [MADlib](https://dl.acm.org/doi/10.14778/1687553.1687576) project, which exercised some of the same challenges.

Our early efforts in language design were ad-hoc extensions of datalog to networking, including [NDlog](https://dl.acm.org/doi/10.1145/1142473.1142485) for routing, and [Overlog](https://dl.acm.org/doi/10.1145/1095810.1095818) for overlay networks. As my ambitions expanded to designing a general-purpose distributed programming, my PhD students (an awesome trio of [Peter Alvaro](https://people.ucsc.edu/~palvaro/), [Tyson Condie](https://dl.acm.org/profile/81314493838) and [Neil Conway](https://www.neilconway.org/)) forced a pause. &quot;Why,&quot; they asked, &quot;would we design a new language before we try building things in the previous language?&quot; So, a bit painfully, they built a new Overlog runtime in Java, and reimplemented large chunks of Hadoop with it in a project we called [BOOM Analytics](https://dl.acm.org/doi/10.1145/1755913.1755937). Tyson also built an Overlog optimizer in Overlog called [Evita Raced](https://dl.acm.org/doi/10.14778/1453856.1453978).

Based on those experiences, we finally nailed our formal semantics with the [Dedalus](https://dl.acm.org/doi/10.1007/978-3-642-24206-9_16) language, which made time and space first class logical citizens, and allowed for the development of a formal model theoretic semantics. Dedalus was still a variant of datalog, which made it nice and clean for formal reasoning, but awkward for developers‚Äîits syntax was unfamiliar, the tooling was minimal, and the programming model felt restrictive for common tasks.

### Practical languages for distributed programming: Bloom and Hydro

Our first *practical* distributed programming language was [Bloom](https://bloom-lang.net), which stepped away from logic programming to embrace functional syntax and algebraic dataflow. Bloom was the first of our languages that was actually pretty fun to program in (though I was one of the few who actually had that experience!) Just as the Bloom PhD students were graduating and moving on to new challenges, I got distracted by adventures in startup land working on visualization, AI and program synthesis for data wrangling (see below). That put this agenda on the back burner for almost 10 years.

Over the past 5 years or so, a team of us has returned to attack high-level distributed programming with vigor‚Äîbuoyed by renewed community interest in correctness, safety, and expressiveness, and a broader ecosystem shift toward systems languages like Rust. Our [Hydro](https://hydro.run) project is a serious effort to deliver on the agenda of general-purpose distributed programming, with new depth and relevance in the Rust ecosystem. This is both a passion project for me, and a serious software package targeted at real developers. I&apos;m sure I&apos;ll be blogging a lot about Hydro and related topics here over time, so I&apos;ll end the discussion here with that.

### CALM, distributed computing and coordination avoidance

I got started on high-level language design for the opportunity to demonstrate optimization opportunities. But it turned out that many of the ideas and lessons that arose were more about semantics than performance.

Just as [language shapes how we think about the world](https://en.wikipedia.org/wiki/Linguistic_relativity), programming languages shape how we think about computing. So maybe it&apos;s no surprise that designing new programming languages helped my group see things differently and ask new questions.

In my early days working on distributed computing, I got restless with that community&apos;s interests in optimizing protocols for tasks like consensus and fault tolerance. As an outsider, I was struck by how much complexity the distributed systems community was willing to embrace for relatively modest performance gains. My background in databases had trained me to seek orders-of-magnitude improvements, so I found myself wondering whether all that protocol engineering was truly essential. Rather than diving into consensus and coordination, I leaned toward avoiding them altogether. How far could I get without them?

That turned out to be a deep question, one that for some reason nobody had asked or answered in the literature. Our experience in Dedalus with &quot;fixing&quot; the bugs in ND/Overlog gave me a big hint: our use cases didn&apos;t need distributed systems coordination *if they were monotone*! (Roughly speaking, monotonicity means that once something becomes true, it stays true‚Äîan idea that‚Äôs easy to recognize in datalog languages but nearly invisible in imperative ones. That&apos;s the benefit of seeing differently!). The more I thought about it, this felt like a deeper insight, a case of *if and only if*. This grew into the [CALM Conjecture](https://dl.acm.org/doi/10.1145/1860702.1860704) which I presented in a keynote at PODS 2010, and which was first proven as the [CALM theorem](https://dl.acm.org/doi/10.1145/3369736) in 2011.

Even outside the CALM formalisms, monotonic thinking has informed a lot of systems work in my group‚Äîfrom the super-fast [Anna](https://dl.acm.org/doi/abs/10.1109/TKDE.2019.2898401) [autoscaling](https://www.vldb.org/pvldb/vol12/p624-wu.pdf) key-value store, to the [Cloudburst](https://dl.acm.org/doi/10.14778/3407790.3407836) stateful serverless platform, to the work [Peter Bailis](http://www.bailis.org/) led on [coordination avoidance for database transactions](https://dl.acm.org/doi/10.14778/2735508.2735509).

Perhaps most surprising was what [Michael Whittaker](https://mwhittaker.github.io/) showed in his PhD work with me: that coordination avoidance techniques could be leveraged to *scale coordination protocols themselves*. That twist led us to [Compartmentalized Paxos](http://www.vldb.org/pvldb/vol14/p2203-whittaker.pdf), and later to a set of [compiler optimizations in Hydro](https://dl.acm.org/doi/10.1145/3639257) that bring these ideas full circle.

And yes, that last bit shows that in the end I didn&apos;t avoid coordination research after all. But the will to procrastinate led to interesting exploration and invention along the way. I&apos;ve had this lesson on my web page since the 1990s:

&gt; &quot;Laziness in doing stupid things can be a great virtue&quot; -- James Hilton, *Lost Horizon*

My corollary might be this: procrastinating known smart things can also be a virtue!

### Human/AI collaborations in Data Wrangling

Early in my career, I got excited about building intuitive, interactive systems that let people explore data fluidly‚Äîa kind of game-like experience for analysis. (Yes, I too got interested in computer science via video games.) This theme started with my work on [Online Aggregation](https://dl.acm.org/doi/abs/10.1145/253260.253291), and continued with a range of efforts in interactive data manipulation.

Inspired by a suggestion from the great [Mike Carey](https://en.wikipedia.org/wiki/Michael_J._Carey_(computer_scientist)) during a seminar at Berkeley, [Vijayshankar Raman](https://www.linkedin.com/in/vijayshankar-raman-95363a/) and I began exploring interactive visual interfaces for data cleaning in the [Potter&apos;s Wheel](https://dl.acm.org/doi/10.5555/645927.672045) project. This work was motivated by a recurring theme I saw in both academia and failed startup ventures like Cohera and Swivel: people get stuck on mundane but necessary data transformation tasks. This was especially frustrating for quantitative professionals without programming backgrounds‚Äîan audience that computer science had largely underserved at the time.

After many years‚Äîand after significant progress in the field of data visualization‚ÄîI had the opportunity to collaborate for the first time with the amazing [Jeff Heer](https://en.wikipedia.org/wiki/Jeffrey_Heer), who was then a rising star. The timing was serendipitous: enough had changed in the field that it felt like the right moment to return to the Potter&apos;s Wheel vision with new tools and energy. We were both excited to pick up where that work had left off. We recruited [Sean Kandel](https://www.linkedin.com/in/seankandel/) away from high frequency trading to enroll in the graduate program with Jeff at Stanford, and he built [Wrangler](https://dl.acm.org/doi/10.1145/1978942.1979444) and [Profiler](https://dl.acm.org/doi/10.1145/2254556.2254659) as vehicles for new ideas in this space, which included an embrace of AI assistance. Sean also kicked us into entrepreneurial mode, and we founded [Trifacta](https://en.wikipedia.org/wiki/Trifacta) to commercialize the work. This turned into a 10-year startup journey‚Äîone that brought new collaborators, new skills, and a crash course in navigating industry shifts. We rode the Big Data wave in and out, and eventually found ourselves in the SaaS era, helped along by Google white-labeling Trifacta as *Google Cloud Dataprep*. That move pushed us further into the future than we might have gone on our own.

Trifacta was very early in exploring questions that are now *au courant* in the LLM era: how do we design environments for humans to collaborate with AI on code and data? Our models and inference quality at the time were far more primitive, relying on heuristics and simple statistical techniques. But many of the UX ideas we explored remain strikingly relevant: empowering users to visually detect data quality issues, interact directly with data visualizations and grids, receive AI suggestions as both code and visual feedback, and iterate rapidly. What has changed is the sharpness of inference; what hasn&apos;t changed is the need to guide and constrain it. Whether the AI is 90% right or 75% right, it still needs to be scaffolded for humans to quickly evaluate and steer the process. These experiences continue to shape how I think about designing AI-powered developer tools‚Äîespecially when it comes to interaction models, scaffolding, and trust. I wrote about our broad ideas in this space in the paper on [Predictive Interaction](https://idl.cs.washington.edu/files/2015-PredictiveInteraction-CIDR.pdf) and the Guide/Decide loop we were exploring in Trifacta. More recently, Berkeley&apos;s [EPIC Data Lab](https://epic.berkeley.edu) was conceived in part based on this experience, and my colleagues there continue to push in many related directions regarding low-code data management.

If you squint, this is another attack on high-level programming models‚Äîin this case &quot;low code&quot; approaches for non-programmers. In that lens, Trifacta was a low-code environment for doing AI-assisted program synthesis of data wrangling scripts. I fully expect that lessons from Wrangler, Trifacta, and Predictive Interaction will influence how we approach LLM-based assistance in Hydro, though Hydro is targeting more technical software engineers and is therefore less data-centric. I bet I&apos;ll have more to say on that front in the coming years.

### ... and so much more

It&apos;s hard to omit so many other topics that I&apos;ve worked with folks on over the years‚Äîespecially because many of them were the work of amazing students and colleagues who I haven&apos;t had a chance to shout out to! I keep a [list of my PhD students](https://dsf.berkeley.edu/jmh/student.html) online. For the research topics, I&apos;ll add an appendix of sorts to the bottom of this post.

## Moving Forward

As I look ahead, I expect to dig even deeper into the Hydro agenda. On the pragmatic front, the codebase is maturing and ready to be tested in the wild‚Äîso it&apos;s time to find bold, high-impact use cases that will stretch our ideas and tools. On the research side, we&apos;re just beginning to scratch the surface of what&apos;s possible. One especially exciting direction is exploring how we can deliver a fundamentally new programming model for distributed systems in the era of AI-assisted development.

You can expect more posts here about those core Hydro themes, as well as the tangents and side quests that keep things interesting‚Äîboth the breakthroughs and the frustrations. As Hydro transitions off campus, I may find myself with even more reason to document the journey. Either way, there‚Äôs a lot to say‚Äîand I‚Äôm looking forward to sharing it.

Thanks for reading. Onward!

## Topics for Another Day

It‚Äôs hard to write a recap like this without feeling the limits of the form. Nearly everything I‚Äôve worked on has been deeply collaborative, and there are far more colleagues and students I admire than I‚Äôve had space to name here. The topics and shoutouts above are a sampling, not a ranking‚Äîand many important threads didn‚Äôt make the main cut simply for reasons of narrative flow or space.

In that spirit, here are a few more topics I‚Äôve worked on that continue to inform how I think about computing today:

- The **[Generalized Search Tree (GiST)](https://gist.cs.berkeley.edu/)** remains a core extensible indexing framework in PostgreSQL and powers spatial extensions like PostGIS. This work also led me into [Indexability Theory](https://dl.acm.org/doi/abs/10.1145/505241.505244) with my longtime mentor [Christos Papadimitriou](https://en.wikipedia.org/wiki/Christos_Papadimitriou).
- **Adaptive query processing of data streams**: Our work on [Eddies](https://dl.acm.org/doi/10.1145/335191.335420), [FLuX](https://dl.acm.org/doi/10.5555/894174), and the [TelegraphCQ](https://telegraph.cs.berkeley.edu/) project helped shape my thinking on stream-centric computing, a topic that is becoming increasingly relevant to general-purpose programming. The Telegraph team members went on to have broad impact across the database industry.
- **Peer-to-peer computing**: The [PIER](https://pier.cs.berkeley.edu/) project emerged during the early-2000s p2p wave. While the hype receded, the architectural ideas lingered. PIER got me thinking about the common ground between querying, indexing, routing, and overlay networks‚Äîcomponents that all play roles in orchestrating distributed data and computation across space and time.
- **Sensor networks and probabilistic inference**: [TinyDB](https://telegraph.cs.berkeley.edu/tinydb/) shaped my early thinking about high-level programming of low-level devices, long before &quot;IoT&quot; was a thing. That line of work evolved into a collaboration with [Carlos Guestrin](https://guestrin.su.domains/) on distributed probabilistic inference‚Äîand helped pique my interest in AI after a discouraging first impression back in the era of expert systems and AI winter.
- **Metadata and data context**: Our [Ground](https://www.ground-context.org/) project explored lineage and metadata‚Äîi.e. *data context*‚Äîin our increasingly disaggregated era. Though we moved on, some of its ideas live on in [Datahub](https://datahubproject.io), thanks to our collaborator [Shirshanka Das](https://www.linkedin.com/in/shirshankadas).
- **Provenance for ML pipelines**: [Rolando Garcia](https://rlnsanz.github.io/) did his thesis work with us on [Flor](https://github.com/ucbrise/flor), a system for *hindsight logging* in long-running training jobs. He continues to push this space forward‚Äîsee his [recent piece](https://arxiv.org/abs/2408.02498) for where it‚Äôs going next.
</content:encoded></item><item><title><![CDATA[Context for a New Home]]></title><description><![CDATA[Time to get blogging again. After a long run with Data in Beta, it's nice to have a fresh start. WordPress was feeling clunky, and over time‚Ä¶]]></description><link>https://jhellerstein.github.io/blog/new-home/</link><guid isPermaLink="false">https://jhellerstein.github.io/blog/new-home/</guid><pubDate>Sun, 27 Apr 2025 00:00:00 GMT</pubDate><content:encoded>
Time to get blogging again. After a long run with [Data in Beta](https://databeta.wordpress.com/), it&apos;s nice to have a fresh start. WordPress was feeling clunky, and over time the title took on unintended connotations. So I‚Äôm starting over‚Äîlighter, cleaner, and more grounded here on GitHub Pages.  

The ideas won‚Äôt be any more ‚Äúfinished‚Äù than before, but it feels like a good time to shed some baggage and keep moving.  

I&apos;ll still be blogging mostly about thoughts that come up in research and development with my team.  
If you&apos;re into programming, computation, data management, or distributed systems,  
you might find things here to interest you over time.  

---

## Research Roots

To set some context for what you&apos;ll find on this blog, here&apos;s a bit about where I‚Äôm coming from‚Äîintellectually and professionally.  

I was trained as a database researcher back in my salad days. Out of college, I interned with the storied database group at IBM Almaden‚Äîthe same team who brought us System R, which begat R*, which begat Starburst, the project I worked on.  

I then did my MS with the amazing Postgres team at Berkeley, and continued working on Postgres with them as I did a PhD with the famed Wisconsin database mafia.  

In retrospect, I was very fortunate to do a tour of duty with each of the most influential database groups of the time. I learned a ton.  

During that training I met some outsized personalities and grew a thicker skin, which has undoubtedly had both positive and negative impacts on my professional life. That said, all my mentors were incredibly kind and supportive to me personally, and I&apos;ll always be paying forward their influences‚Äîespecially [Meichun Hsu](https://www.linkedin.com/in/meichun-hsu-0a72968), [Hamid Pirahesh](https://www.linkedin.com/in/hamid-pirahesh-38368010/), [Mike Stonebraker](https://en.wikipedia.org/wiki/Michael_Stonebraker), and [Jeff Naughton](https://en.wikipedia.org/wiki/Jeffrey_Naughton).  

---

## The Benefits of a Database Upbringing

Database research was‚Äîand still is‚Äîmy home research community. It&apos;s a great space: a cross-cutting area of computing that has, from its beginnings, spanned academia and industry, theory and practice.  

Data management provides a context to work on pretty much every computing topic imaginable. But database folks see the world of computing a bit differently: our primary focus is on the data that moves around, rather than the silicon resources of a computer. This often frees us up to take a broader view.  

There&apos;s a meme in the &quot;Systems&quot; community: for any given topic, someone says ‚ÄúI think database people already solved that problem.‚Äù  
And y‚Äôknow ‚Ä¶ it&apos;s not wrong! üôÇ  

DB folks were among the first in software to tackle service-oriented computing at scale, with correctness and fault tolerance guarantees, and an eye toward serving a wide range of users‚Äînot just hobbyists and hackers.  

The goalposts have shifted since the 1970s, of course, and sometimes being *early* to a technology can be a liability in the business world. But much less so in research!  

It&apos;s kind of amazing how prescient the DB folks were in the 1970s and 1980s (before my time!) about the problems worth solving in computer science. And it&apos;s not just the applied folks‚Äîthere&apos;s also a ton of database theory work that keeps coming back in new contexts.  

---

## Cross-Pollination

Over the years, I‚Äôve had the good fortune to collaborate with friends from all corners of computing: experts in distributed systems, programming languages, HCI, AI, networking, and theory.  

I&apos;ve always liked working with people who can teach me new things, and I enjoy having a broad portfolio of topics to keep me curious.  

Cross-area collaboration pulls you away from the center of your home field‚Äîand on the whole, I‚Äôve been glad about that. Many of the most interesting places are away from the center.  

---

## Outside the Box

Topic areas aside, I generally prefer to work on problems that most folks are *not* working on.

Hot topics drive scientists to race for discovery. Lots of people like racing‚Äîespecially because the fastest racer gets a big medal! But in most cases, if the winner had tripped along the way, someone else would have replaced them with no appreciable difference in outcome.  

I find that highly demotivating, particularly in a field where the main goal is innovation.

I don‚Äôt like to race. I‚Äôd rather explore and invent.  

---

## Coming Up

In the next post, I‚Äôll dig into some of the research that‚Äôs grown out of this perspective‚Äîranging from language design and distributed consistency to data visualization, AI-based systems and beyond.
</content:encoded></item></channel></rss>